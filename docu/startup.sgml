<chapter id="startup">
  <title>Starting up programs</title>
<!--
  <para>
    In general, starting up a parallel program is a non-trivial task if
    it should be done reliably, fast and stable. Furthermore the job
    of controlling the parallel task and cleaning up in case of any
    failure is even harder.
  </para>
-->
  <para>
    &ps;'s management and task handling facilities solve all of the
    nasty problems seen on a compute cluster while handling parallel applications.

    &ps; implements a
    fast and robust startup mechanism for parallel tasks. In addition the
    &ps; daemons running on each node of the cluster control the various
    remotely started <link linkend="process">processes</link> forming the
    <xref linkend="task"/> in a combined effort. They also clean up the whole task if
    one of the processes dies unexpectedly.
  </para>
  <para>
    Within this chapter it is assumed that an executable ready to run on a &ps;
    cluster does already exist. It might have been build by the end-user using the MPI
    framework as described in <xref linkend="mpi"/>, have been provided by an
    independent software vendor or can be a native &ps; application only making
    use of the low-level communication and management functions as described in
    <xref linkend="native_apps"/>. Anyhow, it has to make use of the &ps;
    management library as described in the <ulink url="../api/index.html"
      type="dontshow">API reference</ulink>.
  </para>
  <para>
    This chapter describes how to start a &ps;
    aware executable, how the processes will be distributed within the cluster,
    how to control the distribution strategy used by &ps; and how the mechanism
    of input and output redirection works.
  </para>
  <section>
    <title>Spawning processes</title>
    <para>
      Starting a parallel task consisting of N processes is as simple
      as executing:
    </para>
    <programlisting>  /some/path/to/program -np N <optional><replaceable>args</replaceable></optional></programlisting>
    <para>
      Where <replaceable>args</replaceable> are one or more further arguments
      that will be passed to each instance of <filename>program</filename> within
      the cluster.
    </para>
    <para>
      Alternatively, the command <filename>mpirun</filename> can be used to
      start up a parallel task:
    </para>
    <programlisting>  mpirun -np N /some/path/to/program <optional><replaceable>args</replaceable></optional> </programlisting>
    <para>
      For more details, refer to <xref linkend="mpirun"/>.
    </para>
    <para>
      Given the default settings of &ps; the N instances of
      <filename>program</filename> will be distributed to N free processors in the
      cluster. 
<!--
      If not that much processors are available, the N least used
      processors, i.e. the processors running the least number of processes
      controlled by &ps;, will be used.
-->
    </para>
    <note>
      <para>
        The command <filename>/some/path/to/program</filename> has to exist
        and must be executable for the user on each node. 
      </para>
      <para>
        Likewise the current working directory must be accessible on
        each node.  Otherwise, the users <filename>HOME</filename>
        will be used as current working directory, and a warning will
        be issued.  This may lead to unexpected results.
      </para>
    </note>
    <para>
      Let's have a brief look behind the scenes to understands what is
      happening while starting a parallel task, i.e. when distributing the
      processes forming the task within the cluster.
    </para>
    <para>
      For &ps; versions up to 4.0.6,
      the locally started process (which is the process actually
      created when <filename>/some/path/to/program</filename> is executed)
      connects to the local
      &ps; daemon <citerefentry>
	<refentrytitle>psid</refentrytitle>
	<manvolnum>8</manvolnum>
      </citerefentry>, registers there and request for a list of available
      nodes supporting certain features. This list of nodes then is evaluated
      and sorted according to the requirements that will be discussed in <xref
	linkend="spawn_strategy"/> and the <xref linkend="environment"/> manual
      page. From this, now sorted, list of nodes the process will pick one
      after the other in a round robin scheme and request the &ps; daemon on
      that node to start another instance of <filename>prog</filename>, i.e. to
      spawn a process being part of the parallel task. This will happen N
      times, which means that <emphasis role="bold">all</emphasis>
      <quote>working</quote> processes of the parallel task will be spawned
      ones. As a last step the first locally started process will convert to an
      I/O handling process, the so called <xref linkend="loggerfull"/> process.
      The I/O handling mechanisms will be discussed in detail in <xref
	linkend="io_redirect"/>
    </para>
    <para>
      For &ps; version 4.1 and up, the locally started process (which
      is the process actually created when
      <filename>/some/path/to/program</filename> is executed) connects
      to the local &ps; daemon 
      <citerefentry>
        <refentrytitle>psid</refentrytitle> 
        <manvolnum>8</manvolnum>
      </citerefentry>, issuing a spawn request. This request, 
      including the number of processes and other necessary information,
      is forwarded to the <emphasis>master
      node</emphasis>, which will provide a temporary node list.
      The local process will actually startup the remote processes
      using the psid on the particular node and will afterwards
      convert to an I/O handling process, the so called <xref
      linkend="loggerfull"/> process.
      <!-- TODO: fg: ok??? -->
    </para>

    <para>
      Of course the distribution of the processes building the parallel task
      can be controlled in order to match the site's policy or tailor
      the process placement for a given algorithm.
      Because of the huge number of different types of applications
      running on a cluster &ps; provides far reaching features to
      control the distribution and spawning process.
<!--
      Since the field of possible applications running on
      clusters is very wide and the variety of users emploing clusters is
      similary large the possibility of detailed calibration of the spawning
      strategy has to be provided by a cluster management system.
-->
      The manual page <xref linkend="spawning"/> will discuss this in detail.
    </para>
  </section>

  <!-- =============== Process placement ======================= -->
  <section id="spawn_strategy">
<!--    <title>Spawning strategies</title> -->
    <title>Placing processes onto nodes</title>
    <para>
      The placement of processes for a parallel task within &ps; can be
      biased by various environment variables. Setting this variables is
      optional, if not set, the default behaviour will make sure, that
      the usability of the cluster will not be jeopardized.
    </para>
    <para>
      The user can predefine lists of nodes, can request exclusive
      use or overbooking of nodes, may change sorting criterias for
      node lists, or even may control process neighborhood on SMP
      nodes. Consequently, it is possible to precisely adjust the process
      distribution within the cluster to algorithms implemented and
      thus gain a maximum of performance out of the parallel task.
    </para>
    <para>
      In addition, the administrator may define partitions reserved
      for users or group of users.
    </para>
    <para>
      The manual page <xref linkend="spawning"/> will discuss all this
      parameters in detail.
    </para>
  </section>

  <section id="io_redirect">
    <title>Redirecting standard input and output</title>
    <para>
      The redirection of standard input, output, and error output
      (file descriptor 0, 1 and 2) is done automatically
      without any further user interaction. 
      In particular, the following actions are performed:
    </para>
    <itemizedlist>
      <listitem>
	<para>The originally started process, which is the root
          process (first) of the parallel
	  task, converts to a so called <xref linkend="loggerfull"/> process
	  after spawning all child processes. It receives output from the
	  spawned processes, or more precisely from the controlling <xref
	    linkend="forwarder"/> process, and dispense it to its final
	  destination. This might be the (virtual) terminal from which the root
	  process was started or the file to which the output of the root
	  process is redirected.
	</para>
      </listitem>
      <listitem>
	<para>
          The child processes are started by the &ps; daemon
          <citerefentry> <refentrytitle>psid</refentrytitle>
          <manvolnum>8</manvolnum> </citerefentry> on request of the
          root process of the parallel task via a so called <xref
          linkend="forwarderfull"/> process.  These forwarder
          processes control the spawned child process and handle all
          input/output produced. This output than is forwarded to the
          <xref linkend="logger"/> process on the starting node which
          will send it to its final destination.
	</para>
      </listitem>
    </itemizedlist>
    <para>
      Input data is handled in a quite similar way. The <xref
      linkend="loggerfull"/> process reads the input from the
      (virtual) terminal or the file from which the input is
      redirected and sends it to a forwarder. The default action is to
      send it to the forwarder of the process with rank 0. This
      behaviour might be modified as discussed below.
    </para>
    <para>
      The redirection of input and output can be configured by a
      number of environment variables:
    </para>
    <variablelist>
      <varlistentry>
	<term>PSI_INPUTDEST</term>
	<listitem>
	  <para>Forward all input to the process with rank
	    <replaceable>rank</replaceable> defined by this variable.
	  </para>
	  <para>
	    The rank of a process is unique withiin a parallel task. The rank of a
	    process can be determined via the <citerefentry>
	      <refentrytitle>PSE_getRank</refentrytitle>
	      <manvolnum>3</manvolnum>
	    </citerefentry> library function. The rank of a process is
	    identical to the MPI rank within the
	    <option>MPI_COMM_WORLD</option> context which can be identified
	    using the <citerefentry>
	      <refentrytitle>MPI_Comm_rank</refentrytitle>
	      <manvolnum>3</manvolnum>
	    </citerefentry> library call.
	  </para>
	  <para>
	    The default is to forward all input to the process with rank 0.
	  </para>
	</listitem>
      </varlistentry>
      <varlistentry>
	<term>PSI_SOURCEPRINTF</term>
	<listitem>
	  <para>
            If this environment variable is defined, each fraction of
            output is prepended with a tag providing the rank of the
            process that was producing it. If the <xref
            linkend="loggerfull"/> is put into the verbose mode using
            the <envar>PSI_LOGGERDEBUG</envar> environment variable,
            also the length of the output fraction to print is
            shown.
	  </para>
	</listitem>
      </varlistentry>
      <varlistentry>
	<term>PSI_NOMSGLOGGERDONE</term>
	<listitem>
	  <para>
            Define this environment variable in order to suppress the
            message
          </para>
	  <programlisting>  PSIlogger: done</programlisting>
          <para>
	    produced by the <xref linkend="loggerfull"/> process
            before exiting. This happens when all <xref
            linkend="forwarderfull"/> have closed their connections to
            the logger.
	  </para>
	</listitem>
      </varlistentry>
      <varlistentry>
	<term>PSI_LOGGERDEBUG</term>
	<listitem>
	  <para>
            Turn on verbose mode for the <xref linkend="loggerfull"/>
            process.  This will produce messages about connecting and
            detaching forwarders, received output, sent input and
            received signals.
	  </para>
	  <para>
            This variable is intended for internal use only.
	  </para>
	</listitem>
      </varlistentry>
      <varlistentry>
	<term>PSI_FORWARDERDEBUG</term>
	<listitem>
	  <para>
            Turn on verbose mode for the <xref
            linkend="forwarderfull"/> process. This will print
            information about received input, output and signals.
	  </para>
	  <para>
            This variable is also intended for internal use only.
	  </para>
	</listitem>
      </varlistentry>
    </variablelist>
  </section>

  <section id="user_environment">
    <title>Spawning the environment</title>
<!--
    <para>
      Beside the possibility to redirect I/O of remotely spawned processes one
      might also want to be able to affect the environment of these processes.
      With &ps; it is possible to do so by setting the environment variable
      <envar>PSI_EXPORTS</envar> to an appropriate value.
    </para>
    <para>
      <envar>PSI_EXPORTS</envar> has to contain a comma separated list of
      names of environment variables. Then all processes spawned using &ps;
      will get an environment containing the listed environment variables set
      to the actual value within the context of the spawning process.
    </para>
    <para>
      Per default the environment variables <envar>HOME</envar>,
      <envar>USER</envar>, <envar>SHELL</envar> and <envar>TERM</envar> will be
      spawned. The spawning of these environment variables will always be done,
      i.e. it is not possible to suppress this action independently of the
      value of <envar>PSI_EXPORTS</envar>.
    </para>
-->
    <para>
      Another important task while spawing parallel applications in a
      cluster is to setup a proper environment for the newly created
      processes on each node. 
    </para>
    <para>
      &ps; by default exports only a limited set of environment
      variables to newly spawned processes. These variables are:
    </para>
    <itemizedlist>
      <listitem>
        <para> <envar>HOME</envar> </para>
      </listitem>
      <listitem>
        <para> <envar>USER</envar> </para>
      </listitem>
      <listitem>
        <para> <envar>SHELL</envar> </para>
      </listitem>
      <listitem>
        <para> <envar>TERM</envar> </para>
      </listitem>
    </itemizedlist>
    <para>
      In addition, each currently defined environment variable can be
      exported to spawned processes by adding the variable name to
      the special &ps; variable <envar>PSI_EXPORTS</envar>.
      E.g., within a Bourne shell (or look alike), the commands
    </para>
    <programlisting>  PSI_EXPORTS=&lt;Variable1&gt;,&lt;Variable2&gt;,...
  export PSI_EXPORTS</programlisting>
    <para>
      will export the variables <envar>Variable1</envar> and
      <envar>Variable2</envar> to all processes.
      Accordingly, for a <command>csh</command> based environment, the
      command
    </para>
    <programlisting>  setenv PSI_EXPORTS &lt;Variable1&gt;,&lt;Variable2&gt;,...</programlisting>
    <para>
      will export this variables to all subsequent parallel tasks.
    </para>
    <note>
      <para>
        Beside these variables, additional variables might be
        inherantly set for remote processes by the 
        <citerefentry>
          <refentrytitle>inetd</refentrytitle> <manvolnum>8</manvolnum>
        </citerefentry>
        , e.g. <envar>PATH</envar> or <envar>HOSTNAME</envar>.
      </para>
    </note>
    <para>
      For a complete list of environment variables known to &ps;,
      refer to
      <citerefentry>
        <refentrytitle>ps_environment</refentrytitle>
        <manvolnum>5</manvolnum> 
      </citerefentry>.
    </para>
  </section>

  <section>
    <title>Starting up serial tasks</title>
<!--
    <para>
      &ps; is even capable to manage the startup of serial jobs that are not
      &ps; aware somewhere inside the cluster. Within this scenario &ps; takes
      care concerning the load balancing between different jobs running in the
      cluster, the remote startup procedure and the redirection of input and
      output data.
    </para>
-->
    <para>
      &ps; is capable of starting serial tasks, therefore tasks not
      parallelized with MPI, within the cluster. All actions described
      in section 
      <xref linkend="spawn_strategy"/>, 
      <xref linkend="io_redirect"/> and 
      <xref linkend="user_environment"/>
      are also considered for this type of tasks. 
    </para>
    <para>
      In order to start the serial program <filename>&lt;program&gt;</filename>
      somewhere within the cluster, simply execute
    </para>
    <programlisting>  psmstart &lt;program&gt; <optional><replaceable>args</replaceable></optional></programlisting>
    <para>
      where <replaceable>args</replaceable> are the arguments that shall be
      passed to <filename>&lt;program&gt;</filename>.
    </para>
    <para>
      Depending on the settings of the environment variables as
      discussed in the previous sections,
      <filename>&lt;program&gt;</filename> will be started on a
      distinct node of the cluster. Parts of the current environment
      will eventually be passed to this node, too. Also the input and
      output is forwarded correctly to and from the remotely started
      process.
    </para>
    <para>
      Starting serial processes using the &ps; management facilities
      offers a couple of advantages:
    </para>
    <itemizedlist>
      <listitem>
	<para>
          Load balancing performed by &ps; towards serial processes
          results in a much better usage of the whole cluster in
          contrast of distributing the jobs manually.
	</para>
      </listitem>
      <listitem>
        <para>
          The user does not have to deal with the question, which node
          should be used for a particular job. &ps; takes care about
          deciding where (and when) to run the task.
        </para>
<!--
	<para>On the other hand this allows the end-user to just start serial
	  jobs from the front-end machine <quote>somewhere</quote> within the
	  cluster without having to deal with the question, on which node it
	  will be started actually. &ps; takes and solves the jobs of finding
	  an appropritate node within the cluster which e.g. has a sufficiently
	  small load or has no other jobs running.
	</para>
-->
      </listitem>
      <listitem>
	<para>
          The possibility of starting serial jobs from the frontend
          machine without having to log on to the node that actually
          runs the jobs enables the system operator to disallow the
          users to log on the nodes in general. This enables much
          better control over the cluster and increases the security
          of the system.
	</para>
      </listitem>
    </itemizedlist>
  </section>

  <section id="startup_queuing">
    <title>Using the &ps4; queuing facility</title>
    <para>
      Beginning with version 4.1, &ps; is able to queue task start
      request if the required resources are currently not available.
      This queuing facility is disabled by default. It can be enabled
      by each user independently. All requests of all users are held
      within one queue and managed on a first-come-first-serve
      strategy. 
      <footnote>
      <para>
        For more sophisticated queuing features, common batch
        systems can be integrated with &ps;, for more details see
        &psag;.
      </para>
      </footnote>
    </para>
    <para>
      If queuing is not enabled, start up requests, which cannot
      immediately be satisfied due to resource constrains, e.g. too
      many CPUs are currently used, will terminate giving the error
      message
    </para>
    <programlisting>  PSI: PSI_createPartition: Resource temporarily unavailable.</programlisting>
    <para>
      To enable the queuing facility, an environment variable called
      <envar>PSI_WAIT</envar> must be defined within the users
      environment:
    </para>
    <programlisting>  PSI_WAIT=""
  export PSI_WAIT</programlisting>
    <para>
      The actual value of <envar>PSI_WAIT</envar> is not considered,
      only the existence of this variable is checked.
    </para>
    <para>
      For task startup and process distribution, more environment
      variables are taken into account, e.g.
      <parameter>PSI_OVERBOOK</parameter>.
      See also <xref linkend="spawn_strategy"/>, <xref
      linkend="spawning"/> and
      <citerefentry>
        <refentrytitle>ps_environment</refentrytitle>
        <manvolnum>5</manvolnum> 
      </citerefentry> .
    </para>
  </section>

  <section id="startup_jobsuspend">
    <title>Suspending &ps4; tasks</title>
    <para>
      Parallel Tasks started by &ps; can be suspended by sending the
      signal <constant>SIGTSTP</constant> to the <xref
      linkend="loggerfull"/> process. The signal will be forwarded to
      all processes of the parallel task and will by default stop the
      processes. 
      To continue, the <constant>SIGCONT</constant> must be sent to
      the <xref linkend="loggerfull"/> process. This signal will also
      be forwarded to all processes of the task.
    </para>
    <note>
      <para>
        The application has to be prepared to handle interrupted
        system calls properly. 
      </para>
    </note>
    <para>
      Depending on the transport protocol in use, tasks can be
      suspended only for a limited period time. If using TCP (HwType
      ethernet), connections may timeout and after sending the
      <constant>SIGCONT</constant> signal, the processes will receive
      I/O errors for this sockets. Using the &ps; protocol
      <emphasis>p4sock</emphasis> will solve this problem, as this
      protocol does not use any timeout features.
    </para>
    <para>
      Suspending a task using the signal <constant>SIGTSTP</constant>
      will also trigger the &ps; queuing facility (see <xref
      linkend="startup_queuing"/>). Depending of the global setting
      of <constant>freeOnSuspend</constant>, CPUs will be reused for
      newly spawned processes. Refer to 
      <citerefentry>
        <refentrytitle>parastation.conf</refentrytitle>
        <manvolnum>5</manvolnum> 
      </citerefentry>.
      <!-- TODO: fg: refer to App. B, suspending tasks -->
    </para>
    <!-- TODO: fg: see N.E.'s paper about task suspension -->
  </section>
</chapter>
  <!-- Keep this comment at the end of the file
  Local variables:
  mode: xml
  sgml-omittag:nil
  sgml-shorttag:nil
  sgml-namecase-general:nil
  sgml-general-insert-case:lower
  sgml-minimize-attributes:nil
  sgml-always-quote-attributes:t
  sgml-indent-step:2
  sgml-indent-data:t
  sgml-parent-document:("userguide.xml" "book" "book" ("title" "bookinfo"))
  sgml-exposed-tags:nil
  sgml-local-catalogs:nil
  sgml-local-ecat-files:nil
  End:
  -->
