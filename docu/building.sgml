<chapter id="native_apps">
  <title>Building native &ps; applications</title>
  <para>
    This chapter describes briefly how to write native &ps; applications,
    i.e. applications that make use of the &ps; low level communication
    libraries and the &ps; management facilities without utilizing the standard
    MPI libraries.
  </para>
  <para>
    In general it is not necessary to write native &ps; applications. Since the
    improvement in communication performance from circumventing the high level
    MPI library is negligible, in almost any scenario the utilization of MPI is
    the recommended way. Additionally MPI guarantees the portability of the
    programs to be developed.
  </para>
  <para>
    Nevertheless the source code of a simple application comes with the
    standard &ps; software distribution. The file is named
    <filename>native.c</filename> and is located in the
    <filename>/opt/parastation/doc/examples</filename> directory. Furthermore a
    <filename>Makefile</filename> can be found in the same directory.
  </para>
  <para>
    Taking a brief look at <filename>native.c</filename> it has to be noticed
    that the startup of the application and the necessary communication
    channels is quite complex. In the case of using MPI all this functionality
    is hidden within the simple <citerefentry>
      <refentrytitle>MPI_Init</refentrytitle>
      <manvolnum>3</manvolnum>
    </citerefentry> function. Thus the programmer does not have to take care
    about the details of the startup process. This is a further argument
    towards using MPI.
  </para>
  <para>
    As already mentioned the usage of the &ps; API is only discussed briefly. A
    detailed description of the various function calls forming the API may be
    found in the <ulink url="../api/index.html" type="dontshow">API
      reference</ulink>.
  </para>
  <section>
    <title>Using the management facility</title>
    <para>
      The &ps; management system is used by native application in order to
      startup the processes of the parallel task and to assure the cleaning up
      by the &ps; daemons in the case of  failure of one or more processes
      within the task.
    </para>
    <para>
      In order to contact the local &ps; daemon and to initialize the &ps;
      library utilized to contact the &ps; management facility, <citerefentry>
	<refentrytitle>PSE_initialize</refentrytitle>
	<manvolnum>3</manvolnum>
      </citerefentry> has to be called. The rank of the actual process then is
      determined using the <citerefentry>
	<refentrytitle>PSE_getRank</refentrytitle>
	<manvolnum>3</manvolnum>
      </citerefentry> function.
    </para>
    <para>
      Based on the rank the decision is made which function is inherited by the
      actual process. Three cases have to be distinguished:
    </para>
    <variablelist>
      <varlistentry>
	<term>rank = -1</term>
	<listitem>
	  <para>The process is the very first process started within the
	    parallel task. Thus it is the root of all further processes. When
	    the startup phase is finished this process will become a &ps;
	    logger process and handle the standard I/O of the parallel task.
	  </para>
	</listitem>
      </varlistentry>
      <varlistentry>
	<term>rank = 0</term>
	<listitem>
	  <para>This is the so called master process of the parallel task. It
	    is spawned by the root process. Its function is firstly to spawn
	    all further processes with rank &gt; 0 and then to act as a normal
	    compute process within the parallel task.</para>
	</listitem>
      </varlistentry>
      <varlistentry>
	<term>rank &gt; 0</term>
	<listitem>
	  <para>These are normal compute processes started by the master
	    process.
	  </para>
	</listitem>
      </varlistentry>
    </variablelist>
    <para>
      These three kinds of processes will be discussed within the next
      sections.
    </para>

    <section>
      <title>The root process</title>
      <para>
	Both functions this process has to fulfill are handled almost
	completely within one function calls. Thus nearly functionality is
	hidden from the users point of view.
      </para>
      <para>
	But first <citerefentry>
	  <refentrytitle>PSE_setHWType</refentrytitle>
	  <manvolnum>3</manvolnum>
	</citerefentry> has to be called in order to specifiy the type of
	communication hardware the parallel task should utilize. This is a kind
	of preconfiguration of the following function call in order to spawn
	further processes.
      </para>
      <para>
	All the rest that has to be done by this process is hidden within the
	<citerefentry>
	  <refentrytitle>PSE_spawnMaster</refentrytitle>
	  <manvolnum>3</manvolnum>
	</citerefentry> function: The master process is spawned and the process
	is converted into a &ps; logger process. This function usually never
	returns.
      </para>
    </section>

    <section>
      <title>The master process</title>
      <para>
	The first action the master process, as any other compute process, has
	to achieve is to register to its parent process via <citerefentry>
	  <refentrytitle>PSE_registerToParent</refentrytitle>
	  <manvolnum>3</manvolnum>
	</citerefentry>. This is done in order to be noticed if the parent
	process exits unexpectedly. The usual behavior is to receive a SIGTERM
	signal in the case that this happens.
      </para>
      <para>
	After it is registered to its parent process further processes, in this
	context called client processes, are going to be spawned. The target is
	to obtain the requested number of processes within the parallel task.
	This is done by calling the <citerefentry>
	  <refentrytitle>PSE_spawnTasks</refentrytitle>
	  <manvolnum>3</manvolnum>
	</citerefentry> function. At this point it has to be remarked that
	usually information has to be passed from the master process to its
	clients. This is due to enable the clients to connect back to the
	master in order to establish the connections used for communication.
	Thus the node and port number of the master process' communication
	interface has to be passed to the <citerefentry>
	  <refentrytitle>PSE_spawnTasks</refentrytitle>
	  <manvolnum>3</manvolnum>
	</citerefentry> function as well.
      </para>
      <para>
	How to get an interface to the low level communication protocols
	provided by &ps; and to fetch information about this interface will be
	discussed within the next section.
      </para>
    </section>

    <section>
      <title>Client processes</title>
      <para>
	The client processes spawned by the master process have to register to
	their parent process, too. Thus they call <citerefentry>
	  <refentrytitle>PSE_registerToParent</refentrytitle>
	  <manvolnum>3</manvolnum>
	</citerefentry> as one of the first actions undertaken after the rank
	is determined.
      </para>
      <para>
	For the further operation of the client processes it is necessary to
	get the information passed by the master process to them. This can be
	reached by using <citerefentry>
	  <refentrytitle>PSE_getMasterNode</refentrytitle>
	  <manvolnum>3</manvolnum>
	</citerefentry> and <citerefentry>
	  <refentrytitle>PSE_getMasterPort</refentrytitle>
	  <manvolnum>3</manvolnum>
	</citerefentry> respectively.
      </para>

      <para>
	All further actions attempted by the master process on the one side and
	the client processes on the other side within the startup phase are
	concerning the establishment of the connections in order to do
	communication. These will be discussed within the next section.
      </para>
      <para>
	After the startup phase all processes of the parallel task will reach a
	mode of normal operation, which usually will cover the actual
	application code. Within the application two kinds of exit mechanisms
	might wanted to be used. On the one hand a error detected within one
	process should result in the end of the whole parallel task. This can
	be reached by using the
	<citerefentry>
	  <refentrytitle>PSE_abort</refentrytitle>
	  <manvolnum>3</manvolnum>
	</citerefentry> function. On the other hand one of the processes might
	have finished all its tasks and want to exit without disturbing the
	other processes within the parallel task. In order to do so,
	<citerefentry>
	  <refentrytitle>PSE_finalize</refentrytitle>
	  <manvolnum>3</manvolnum>
	</citerefentry> has to be called. Afterwards the process might exit
	without shutting down all other processes.
      </para>
    </section>
  </section>
  <section>
    <title>Using the <emphasis role="bold">PSPort</emphasis> interface</title>
    <para>
      In order to actually utilize the low level &ps; communication interfaces
      the <emphasis role="bold">PSPort</emphasis> library has to be used. The
      <emphasis role="bold">PSPort</emphasis> library supplies fast and
      reliable point to point connections to the user.
    </para>
    <para>
      To make use of these connections first of all a port has to be
      opened. All further communication operations will act towards this port.
    </para>
    <para>
      Furthermore information about all the ports of the other processes
      building the parallel task has to be gathered in order to be able to send
      data to this processes. Within the example program this problem is solved
      in the following way:
    </para>
    <orderedlist>
      <listitem>
	<para>Initialize the <emphasis role="bold">PSPort</emphasis> interface
	  using the <citerefentry>
	    <refentrytitle>PSP_Init</refentrytitle>
	    <manvolnum>3</manvolnum>
	  </citerefentry> function within every process.
	</para>
      </listitem>
      <listitem>
	<para>Open a port using the <citerefentry>
	    <refentrytitle>PSP_OpenPort</refentrytitle>
	    <manvolnum>3</manvolnum>
	  </citerefentry> function call. The local port number and the &ps;
	  node ID are determined via <citerefentry>
	    <refentrytitle>PSP_GetPortNo</refentrytitle>
	    <manvolnum>3</manvolnum>
	  </citerefentry> and <citerefentry>
	    <refentrytitle>PSP_GetNodeID</refentrytitle>
	    <manvolnum>3</manvolnum>
	  </citerefentry> respectively. This has to be done within every
	  process, too.
	</para>
      </listitem>
      <listitem>
	<para>The master process, i.e. the process with rank 0, which has
	  spawned all other processes, is going to receive information
	  concerning the local node ID and port number from the client
	  processes. Therefore it has passed its own node ID and port number to
	  the clients through the <citerefentry>
	    <refentrytitle>PSE_spawnTasks</refentrytitle>
	    <manvolnum>3</manvolnum>
	  </citerefentry> function as discussed within the last section.
	</para>
      </listitem>
      <listitem>
	<para>The client processes send the information concerning their local
	  ports, i.e. port number and node ID, to the master process. Therefore
	  they have to determine the node ID and port number of the master
	  process before. This is done by using the <citerefentry>
	    <refentrytitle>PSE_getMasterNode</refentrytitle>
	    <manvolnum>3</manvolnum>
	  </citerefentry> and <citerefentry>
	    <refentrytitle>PSE_getMasterPort</refentrytitle>
	    <manvolnum>3</manvolnum>
	  </citerefentry> function calls repsectively as discussed above.
	</para>
      </listitem>
      <listitem>
	<para>When the master process has completed to receive the information
	  from the client processes it redistributes the collected port and
	  node numbers to all processes.
	</para>
      </listitem>
    </orderedlist>
    <para>
      Now each process has informations about the port number of all other
      processes. These informations can then be used in order to do
      communication operations on a point-to-point basis between all the
      processes forming the parallel task.
    </para>
    <para>
      The actual communication operations utilizing the low level <emphasis
	role="bold">PSPort</emphasis> interface are initiated using the
      <citerefentry>
	<refentrytitle>PSP_ISend</refentrytitle>
	<manvolnum>3</manvolnum>
      </citerefentry> and <citerefentry>
	<refentrytitle>PSP_IReceive</refentrytitle>
	<manvolnum>3</manvolnum>
      </citerefentry> functions. All communication within <emphasis
	role="bold">PSPort</emphasis> is nonblocking. Therefore each
      communication operation has to be tested upon completion. This is done
      via the <citerefentry>
	<refentrytitle>PSP_Wait</refentrytitle>
	<manvolnum>3</manvolnum>
      </citerefentry> function.
    </para>
  </section>
</chapter>
  <!-- Keep this comment at the end of the file
  Local variables:
  mode: xml
  sgml-omittag:nil
  sgml-shorttag:nil
  sgml-namecase-general:nil
  sgml-general-insert-case:lower
  sgml-minimize-attributes:nil
  sgml-always-quote-attributes:t
  sgml-indent-step:2
  sgml-indent-data:t
  sgml-parent-document:("userguide.xml" "book" "book" ("title" "bookinfo"))
  sgml-exposed-tags:nil
  sgml-local-catalogs:nil
  sgml-local-ecat-files:nil
  End:
  -->
