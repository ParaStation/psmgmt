<chapter id="overview">
  <title>Overview</title>
  <para>
    Besides the brief overview on &ps; given within <link
    linkend="intro">the introduction</link> this chapter will give a
    more detailed insight concerning the building blocks forming the
    &ps; system and the main architecure of &ps4;.
  </para>
  <para>
    As already mentioned &ps4; consists of the modules
  </para>
  <itemizedlist>
    <listitem>
      <para>high performance communication subsystem and</para>
    </listitem>
    <listitem>
      <para>the cluster management facility.</para>
    </listitem>
  </itemizedlist>
  <para>
    The communication subsystem of &ps; is composed of a couple of
    libraries and kernel modules. Applications that want to benefit
    from the &ps; communication system have to be build against these
    libraries, except when the TCP bypass is used. 
    Furthermore the cluster nodes running this application
    have to have the kernel module(s) loaded and the shared versions
    of the communication libraries loaded, if used.
  </para>
  <para>
    The management part of &ps; is implemented in a daemon process,
    running on each of the cluster nodes. All these daemons constantly
    gather and interchange information in order to get a unique global
    view of the cluster.  Applications that want to profit from this
    view to the cluster have to talk to this daemons. This is usually
    done via an interface implemented in another library. Thus
    parallel applications have to be linked against this library, too.
  </para>
  <para>
    Both parts of &ps; will be discussed in detail within the next
    sections of this chapter.
  </para>
  <para>
    In order to preserve the high communication bandwidth of the fast network
    to the applications, a strict separation between the application
    related communication traffic
    <!-- TODO: fg: application traffic? -->
    and
    the network traffic caused by administration tasks is made. 
    Therefore, the &ps; communication subsystem is only used
    for the application traffic. The concept of splitting
    the two different types of communication is shown in <xref
      linkend="picture_network"/>.
  </para>

  <figure id="picture_network">
    <!-- TODO: fg: redo picture!!! -->
    <title>The &ps; network setup.</title>
    <mediaobject>
      <imageobject>
	<imagedata scale="80%" fileref="../images/network.png"/>
      </imageobject>
      <imageobject>
	<imagedata scale="66" align="center"
	  fileref="../images/network.pdf"/>
      </imageobject>
      <imageobject role="HTML">
	<imagedata width="75%" align="center"
	  fileref="../images/network.png"/>
      </imageobject>
      <imageobject role="FO-PDF">
	<imagedata width="12cm" align="center"
	  fileref="../images/network.png"/>
      </imageobject>
    </mediaobject>
  </figure>

  <para>
    <!-- TODO: fg: redo this paragraph! -->
    Often both type of communication share the same physical network.
    Especially for Ethernet based clusters, this is a common
    architecture. Albeit this may cause performance problems for some
    parallel applications.
  </para>
  <para>
    As depicted in <xref linkend="picture_network"/> it is possible to have an
    optional frontend machine not connected by the application network to the
    cluster, all the same being fully integrated into the &ps;
    management system. Thus, it is possible to start parallel
    applications on this machine, making it unnecessary for the user
    to login or even being able to login to every cluster node.
    Furthermore, the frontend machine might act as an
    <filename>home</filename>-directory and compile server for the
    users.
  </para>

  <section>
    <title>The communication subsystem</title>

    <para>
      The &ps; high speed communication subsystem supports different
      communication paths and interconnect technologies. Depending on
      the physically available network(s) and the actual
      configuration, these interconnects are automatically selected by
      the communication library.
    </para>
    <section>
      <title>Communication paths</title>
      <para>
        &ps4; currently supports a variety of communication paths
        to transfer application data. Not all paths are always
        available, depending on the physical network(s) and system
        architecture (uni-processor or SMP) installed and configuration and
        licensing defined.
      </para>
      <para>
        The following list shows all currently available interconnects
<!--
        <footnote>
          <para>
            The next version of &ps; will also support Infiniband.
          </para>
        </footnote>
-->
        and protocols, supported by &ps4;. The list also defines the
        order used to select a transport by the process management.
      </para>
      <variablelist>
	<varlistentry>
	  <term>Shared Memory</term>
	  <listitem>
	    <para>
              If two or more processes of a parallel task will run on
              the same physical node, shared memory will be used for
              communication between these processes.  This typically
              happens on SMP nodes, where one process per CPU is
              spawned.
	    </para>
	  </listitem>
	</varlistentry>
	<varlistentry>
	  <term>Infiniband</term>
	  <listitem>
	    <para>
              If Infiniband is available on this cluster, &ps; will
              make use of the vapi driver.
	    </para>
	  </listitem>
	</varlistentry>
	<varlistentry>
	  <term>Myrinet using GM</term>
	  <listitem>
	    <para>
              <!-- TODO: fg: describe -->
              This interconnect is only available, if Myrinet and GM
              is installed on this cluster.
	    </para>
	  </listitem>
	</varlistentry>
	<varlistentry>
	  <term>Ethernet using &ps; protocol p4sock</term>
	  <listitem>
	    <para>
              This is the most effective and therefore preferred
              protocol for Ethernet based clusters. It is based on the
              &ps4; specific protocol p4sock.
	    </para>
	  </listitem>
	</varlistentry>
	<varlistentry>
	  <term>Ethernet using TCP</term>
	  <listitem>
	    <para>
              This is the default communication path.
              In fact, all connections using TCP/IP can be used,
              independent of the underlying network.
	    </para>
	  </listitem>
	</varlistentry>
      </variablelist>

      <para>
        <!-- TODO: fg: more details -->
        Independent of the underlying transport networks and 
        protocols in use, &ps; uses reliable communication.
      </para>
      <para>
        While spawning processes on a cluster, &ps; will decide which
        interconnect can be used for communication. 
      </para>

    </section>

    <section>
      <title>User level communication</title>

      <para>
        Whenever possible, &ps; uses user level communication to
        speedup communication. 
        This concept
	circumvents the operating system in order to do communication and fades
	in the communication hardware into the application's address space.
      </para>
      <para>
        However, this is not always possible. &ps4; uses this approach
        for shared memory, Infiniband and Myrinet based communication.
      </para>
    </section>

    <section>
      <title>Communication interfaces</title>

      <para>
        &ps4; provides several communication interfaces, suitable for
        different levels of functionality and environments.
      </para>
      <variablelist>
	<varlistentry>
	  <term><emphasis role="bold">PSPort</emphasis></term>
	  <listitem>
	    <para>The <emphasis role="bold">PSPort</emphasis> interface is the
	      native low-level communication interface provided by &ps;. Any
	      communication can be done using this interface, although programs
	      using it will not be portable. Thus it is recommended to use a
	      standard interface as MPI discussed below.
	    </para>
	    <para>
	      The main features of the <emphasis role="bold">PSPort</emphasis>
	      interface are:
	    </para>
	    <itemizedlist>
	      <listitem>
		<para>De-/fragmentation of large messages.</para>
	      </listitem>
	      <listitem>
		<para>Buffering of asynchronously received messages.</para>
	      </listitem>
	      <listitem>
		<para>Provision of ports.</para>
	      </listitem>
	      <listitem>
		<para>Selective receive.</para>
	      </listitem>
	      <listitem>
		<para>Thread save.</para>
	      </listitem>
	      <listitem>
		<para>Control Data possible in every message.</para>
	      </listitem>
	    </itemizedlist>
            <para>
              The PSPort interface is encapsulated in the library
              <filename>libpsport.a</filename>. 
            </para>
	  </listitem>
	</varlistentry>

	<varlistentry>
	  <term><emphasis role="bold">TCP bypass</emphasis></term>
	  <listitem>
            <para>
              This Linux kernel extension redirects network traffic
              within a cluster from the
              TCP layer to the &ps; p4sock protocol layer.
              Due to the very small overhead of this protocol,
              this bypass functionality
              increases performance and lowers latencies seen by the
              application.
            </para>
            <para>
              The application does not recognize this redirection of
              network packets and needn't be modified in any way.
              Also, the user does not have to take any special steps
              to use this performance feature.
            </para>
            <note>
              <para>
                This feature of &ps; is still experimental in version
                4.1.1. For details, please contact
                <email>support@cluster-competence-center.com</email>.
              </para>
            </note>
	  </listitem>
	</varlistentry>

	<varlistentry>
	  <term><emphasis role="bold">MPI</emphasis></term>
	  <listitem>
	    <para>This is an implementation of the MPI Message Passing
	      Interface. It is assumed as the standard interface in order to
	      write parallel aplications.
	    </para>
	    <para>
	      Its key features are:
	    </para>
	    <itemizedlist>
	      <listitem>
		<para>Based on MPIch 1.2.5.</para>
	      </listitem>
	      <listitem>
		<para>Synchronous and asynchronous communication.</para>
	      </listitem>
	      <listitem>
		<para>Zero copy communication.</para>
	      </listitem>
	      <listitem>
		<para>Implements the whole 1.2 standard of MPI-1 and parts of
		  MPI-2.</para>
	      </listitem>
	      <listitem>
		<para>Support for MPI-IO to a PVFS parallel filesystem.</para>
	      </listitem>
	    </itemizedlist>
	  </listitem>
	</varlistentry>

	<varlistentry>
	  <term><emphasis role="bold">RMI</emphasis></term>
	  <listitem>
	    <para>This interface enables even Java to profit from the
	      high-performance communication provided by &ps;.
	    </para>
	    <para>
	      This interface is mainly a research project and thus not included
	      within the standard distribution. The corresponding parts of &ps;
	      may be obtained by request from Cluster Competence Center. Please contact
	      <email>support@cluster-competence-center.com</email>.
	    </para>
	    <para>
	      The main features of the <emphasis role="bold">RMI</emphasis>
	      interface are:
	    </para>
	    <itemizedlist>
	      <listitem>
		<para>Enable Remote Method Invocation over all
                supported interconnects.</para>
	      </listitem>
	      <listitem>
		<para>Serialization of objects is provided.</para>
	      </listitem>
	    </itemizedlist>
	  </listitem>
	</varlistentry>
      </variablelist>
    </section>
  </section>

  <section>
    <title>The cluster management facility</title>
    <para>
      In addition to the pure communication tasks that build the primary
      requirements of parallel application running on a cluster, further
      assistance of the runtime environment is needed in order to startup and
      control these applications.
    </para>
    <para>
      The main requirements demanded from a cluster management framework are:
    </para>
    <variablelist>
      <varlistentry>
	<term><emphasis role="bold">Node management</emphasis></term>
	<listitem>
	  <para>The management system should automatically detect usable
	    nodes. The ability to administrate this nodes from a central point
	    of administration has to be provided.
	  </para>
	</listitem>
      </varlistentry>
      <varlistentry>
	<term><emphasis role="bold">Fault tolerance</emphasis></term>
	<listitem>
	  <para>If some nodes are temporarily unavailable due to hardware
	    failure, maintenance or any other reason, the management system
	    must not fail. This situation has to be gracefully treated and
	    normal work with the remaining nodes has to be provided.</para>
	</listitem>
      </varlistentry>
      <varlistentry>
	<term><emphasis role="bold">Process management</emphasis></term>
	<listitem>
	  <para>The management system has to provide a transparent mechanism to
	    start parallel tasks. Load balancing has to be considered within
	    this process. Further requirements to special resources only
	    provided by some nodes have to be taken into account. The actual
	    behavior should be widely configurable.</para>
	</listitem>
      </varlistentry>
      <varlistentry>
	<term><emphasis role="bold">Job control &amp; Error
	    management</emphasis></term>
	<listitem>
	  <para>If some or all processes forming a parallel task fail
	    unexpectedly, the management system has to reset left over
	    processes and clean up the accessed node in order to keep the
	    cluster in a usable state.
	  </para>
	</listitem>
      </varlistentry>
      <varlistentry>
	<term></term>
	<listitem>
	  <para></para>
	</listitem>
      </varlistentry>
    </variablelist>
    <para>
      &ps; fulfills all this requirements by providing a framework for
      cluster-wide job and resource management. The backbone of the &ps;
      management facility is build by the &ps; daemons <citerefentry>
	<refentrytitle>psid</refentrytitle>
	<manvolnum>8</manvolnum>
      </citerefentry> running on every node of the cluster, communicating
      underneath each other constantly. Thus the management system is not
      located on a single node but distributed throughout the whole cluster.
      This design prevents the creation of single points of failure.
    </para>
    <para>
      The main tasks of the &ps; daemons are:
    </para>
    <itemizedlist>
      <listitem>
	<para>recovery and distribution of local informations like the
	  local load values, the state of the locally controlled processes or
	  the condition of local communication interface,
	</para>
      </listitem>
      <listitem>
	<para>status control of the cluster by receiving the information
	  from all other daemons,
	</para>
      </listitem>
      <listitem>
	<para>initialization and the status control of the local
	  communication layer,
	</para>
      </listitem>
      <listitem>
	<para>startup of local processes on request of local or remote
	  processes,
	</para>
      </listitem>
      <listitem>
	<para>control of the locally started processes,
	</para>
      </listitem>
      <listitem>
	<para>transmission of input and output of the locally controlled
	  processes,
	</para>
      </listitem>
      <listitem>
	<para>forwarding of signals to the locally controlled processes and
	</para>
      </listitem>
      <listitem>
	<para>provision of an interface to the &ps; cluster management
	  environment.
	</para>
      </listitem>
    </itemizedlist>

    <section>
      <title>Node management</title>
      <para>
	In order to provide a fault tolerant node management, &ps; uses the
	concept of virtual nodes in contrast to the classical approach of
	handling statical node lists. 
        The idea behind this concept is not to manage nodes by
        lists of hostnames provided by the user, but to provide a pool
        of virtual nodes and request nodes from this list at startup
        time.
<!--
        The main idea of this concept is the fact,
	that nodes are not managed by their hostnames put into some lists by
	the users but to have a pool of virtual nodes. 
-->
        These virtual nodes of
	course represent the physical nodes available. Users
	normally do not request for special nodes but for a set of virtual
	nodes.
      </para>
      <para>
	The main advantage of this concept becomes clear if one or more nodes
	are temporarily unavailable or even down. While the concept of static
	nodelists requires the user to change this lists manually, within the
	setup choosen by &ps; the pool becomes smaller because of the lacking
	nodes. On the other hand the user does not have to change the syntax or
	any configuration in order to get a required number of nodes, at least
	as long as enough nodes are available.
      </para>
      <para>
	Albeit the request of selected nodes is not necessary due to the virtual
	node concept, it is still possible. This
	behavior is quite useful in the case where a virtual partitioning of
	the cluster controlled by an external batch system is desired.
      </para>
      <para>
	Normally the user will request for any N nodes. &ps; then decides which
	nodes are available and fit best to the requirements of the user. These
	nodes will be given to the user and the parallel task will be spawned
	throughout these nodes.
      </para>
      <para>
	Further request for virtual nodes posted by any other user will
	recognize the processes of this parallel task and take them into
	account when processes have to be spawned, too.
        <!-- TODO: fg: describe load balancing -->
      </para>
      <para>
	The details of process distribution within the startup of parallel
	tasks will be discussed in detail in <xref linkend="startup"/>
      </para>
    </section>

    <section>
      <title>Fault tolerance</title>
      <para>
	Within &ps; fault tolerance is given in many ways. First of all the
	concept of virtual nodes provides stability against node failure in the
	sense that the cluster as a whole will work even if several nodes are
	temporarily unavailable.
      </para>
      <para>
	In addition, &ps; offers mechanisms in order to increase the
	reliability and stability of the cluster system. E.g. if a node fails
	while it is utilized by parallel tasks, this task will be shut down in
	a controlled manner. Thus the remaining nodes that where used by this
	task will be cleaned up and released for further jobs.
      </para>
      <para>
	The distributed concept of &ps;'s management facilities makes the
	administration of the cluster feasible even if some nodes are
	down. Furthermore it prevents the emerge of single point of failure
	that would lead to an unusable cluster due to a local failure on just
	one node.
      </para>
    </section>

    <section>
      <title>Process management</title>
      <para>
	The management facility of &ps; offers a complete process management
	system. &ps; recognizes and controls dependencies
	between processes building a parallel task on various nodes of the
	cluster.
      </para>
      <para>
	The process management includes creation of processes on
	remote nodes, control of the I/O channels of these remotely started
	processes and the management of signals across node boundaries.
      </para>
      <para>
	In contrast to the spawning mechanism used by many other cluster
	management systems, i.e. the spawning via a rsh/ssh mechanism, the
	startup of remote processes via &ps; is very fast since the &ps; daemon
	<citerefentry>
	  <refentrytitle>psid</refentrytitle>
	  <manvolnum>8</manvolnum>
	</citerefentry> is constantly in a stand-by mode in order to start
	these processes. No further login or authentification
	overhead is necessary.
      </para>
      <para>
	Since &ps; knows about the dependencies between the processes building
	a parallel task it is able to take them into account. The processes are no
	longer independent but form a task in the same sense as the nodes no
	longer are independent computers but form the cluster as a unique
	system. The fact that &ps; handles distributed processes as a unit,
	plays an important rule especially in the context of job control and
	error management discussed within the next section.
      </para>
      <para>
	Furthermore &ps; takes care that output produced by remote processes is
	forwarded to the intended destination. This is usually the controlling
	<filename>tty</filename> of the I/O handling process, i.e. the process
	that was initially started in order to bring up the parallel task, but
	might also be a normal file the output is redirected to. Input directed
	to the parallel task is forwarded, too. The default is to send it to
	the process with rank 0, but it might be addressed to any process of
	the parallel task.
      </para>
      <para>
	Last but not least &ps; handles the propagation of signals. This means
	that signals send to the I/O handling process will be forwarded to all
	processes of the task.
      </para>
    </section>

    <section>
      <title>Job control &amp; error management</title>
      <para>
	Beside the job control already discussed in the previous section, error
        <!-- TODO: fg: bad english -->
	management plays an important role in order to run a cluster without
	much administrative engagement on a day to day basis. It is
	not acceptable to have to clean up remaining processes of crashed parallel task
	by hand, to lose output due to erroneous processes or to leave the user
	without meaningful error messages after a parallel task has failed.
      </para>
      <para>
	&ps; supports the administrator and the end user within this complex.
	Since the management facility controls all processes that were notified
	towards &ps;, it is capable to take actions in the case that one of the
	processes fails.
      </para>
      <para>
	If an unexpected failure of a process is recognized, all processes
	within the corresponding parallel task will be notified. The parent
	process of the parallel task will be notified in any case, all other
	processes will be signaled only on request.
      </para>
      <para>
	Furthermore all necessary measures will be taken in order to clean up
	the resources that were allocated by the crashed process.
      </para>
      <para>
	Besides the fact that all output that was <quote>on the line</quote>
	when the failure took place will reach its final destination, the user
	will get feedback about the kind of error that crashed the process. This
	feedback is usually of the form <quote>Got
	  <replaceable>sigxxx</replaceable> on node
	  <replaceable>nodenum</replaceable></quote>.
      </para>
    </section>

    <section>
      <title>Integration into existing environments</title>
      <para>
	Beside the inherent management capabilities of &ps; it is prepared for
	easy interaction with more evolved batch systems as e.g. LSF, OpenPBS
	or PBS PRO. This enables a &ps; cluster to get embedded into an existing
	environment or even to build a node within a Grid.
      </para>
      <para>
	The integration of a &ps; cluster into an existing environment relieves
	the end user from learning just another runtime environment.
	Furthermore it makes the use of a cluster more stream lined with an
	existing site policy concerning the use of supercomputing facilities.
      </para>
    </section>
  </section>
</chapter>
  <!-- Keep this comment at the end of the file
  Local variables:
  mode: xml
  sgml-omittag:nil
  sgml-shorttag:nil
  sgml-namecase-general:nil
  sgml-general-insert-case:lower
  sgml-minimize-attributes:nil
  sgml-always-quote-attributes:t
  sgml-indent-step:2
  sgml-indent-data:t
  sgml-parent-document:("userguide.xml" "book" "book" ("title" "bookinfo"))
  sgml-exposed-tags:nil
  sgml-local-catalogs:nil
  sgml-local-ecat-files:nil
  End:
  -->
