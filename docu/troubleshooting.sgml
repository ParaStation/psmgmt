<chapter id="troubleshooting">
  <title>Troubleshooting</title>

  <para>
    This chapter provides some hints to common problems seen while
    installing or using &ps4;. More help will be provided by
    <email>support@par-tec.com</email>.
  </para>
  <variablelist>
    <varlistentry>
      <term>
        Problem: <filename>psiadmin</filename> cannot be started.
      </term>
      <listitem>
      <programlisting>  
  # psiadmin
  PSC: PSC_startDaemon: connect() fails: Connection refused
      </programlisting>
      <para>
        Reason: the local psid could not be connected. Verify that the
        <xref linkend="psid"/> daemon is known to the (x)inetd:
      </para>
      <programlisting>
  # netstat -at | grep psid
  tcp        0      0 *:psid                  *:*      LISTEN
      </programlisting>
      <para>
        If no "listening" socket is reported, check
        <filename>/etc/services</filename> for an entry of the &ps;
        daemon:
      </para>
      <programlisting>
  # grep psid /etc/services
  psid             888/tcp       # ParaStation Daemon start port
      </programlisting>
      <para>
        If this is ok, reload (x)inetd:
      </para>
      <programlisting>
  # kill -HUP &lt;pid of inetd&gt;
      </programlisting>
      <para>
        If everything seems to be ok, check for recent entries within
        <filename>/var/log/messages</filename>. Be aware, the log
        facility can be modified using the
        <parameter>LogDestination</parameter> within the
        config file <filename>parastation.conf</filename>. 
        Look for lines like
      </para>
      <programlisting>
  Mar 24 17:19:12 pan psid[7361]: Starting ParaStation DAEMON
  Mar 24 17:19:12 pan psid[7361]: Protocol Version 329
  Mar 24 17:19:12 pan psid[7361]:  (c) ParTec AG (www.par-tec.com)
      </programlisting>
      <para>
        These lines indicate a normal startup of the psid. Other
        messages may indicate problems found by the psid.
      </para>
      <para>
        Check the configuration using the
        <filename>test_config</filename> utility.
      </para>
    </listitem>
    </varlistentry>

    <varlistentry>
      <term>
        Problem: Nodes shown as "down":
      </term>
    <listitem>
      <para>
        Maybe the node is currently not available (shutdown or
        crashed), or the network connection to this node is not
        available.
      </para>
      <para>
        Try to <filename>ping</filename> this node. If ok, try to
        startup &ps;. From an other node, "add" these node:
      </para>
      <programlisting>  psiadmin> add &lt;nodeid&gt;</programlisting>
      <para>
        Or logged in to this node, run <filename>psiadmin</filename>
        which also starts up the <filename>psid</filename>.
      </para>
      <para>
        Check the logfile <filename>/var/log/messages</filename> on
        this node for error messages. Especially the message
      </para>
      <programlisting>  psid[4457]: MCast: Getting MCast ping from unknown node [3 192.168.1.5(2)]</programlisting>
      <para>
        will point to a miss-configuration of node names within
        <filename>parastation.conf</filename>. Probably nodes using
        more then one network interface are affected. Verify, that
        only nodes "visible" from within the cluster are listed within
        the configuration file.
      </para>
    </listitem>
    </varlistentry>

    <varlistentry>
      <term>
        Problem: After "adding" nodes, they are reported as "up", but
        after a short period of time, they are reported as "down":
      </term>
    <listitem>
      <para>
        Look for "missing multicast license pings" messages 
      </para>
      <programlisting>  psid[7380]: MCast: Ping from LicServer 192.168.1.5 missing [11]</programlisting>
      <para>
        within the
        message file of the nodes. Probably, the license daemon
        running on the frontend node does not send license pings to
        the proper network interface. 
      </para>
      <para>
        Add a multicast route on the frontend to redirect multicast
        messages to the cluster:
      </para>
      <programlisting>	route add -net 224.0.0.0 netmask 240.0.0.0 dev <replaceable>ethX</replaceable></programlisting>
      <para>
        where <replaceable>ethX</replaceable> should be replaced by
        the actual name of the interface connecting to all other
        cluster nodes. In order to enable this route at system
        startup, a corresponding entry has to be added to
        <filename>/etc/route.conf</filename> or
        <filename>/etc/sysconfig/networks/routes</filename>, depending
        on the type of Linux distribution in use.
      </para>
    </listitem>
    </varlistentry>

    <varlistentry>
      <term>
        Problem: Cannot start application:
      </term>
    <listitem>
      <programlisting>
  PSI: PSI_createPartition: Resource temporarily unavailable
      </programlisting>
      <para>
        Check for available nodes and active parallel tasks. Check for
        user or group restrictions.
      </para>
    </listitem>
    </varlistentry>

    <varlistentry>
      <term>
        Problem: Bad performance:
      </term>
    <listitem>
      <para>
        Look for a crystal bowl! 
        <!-- fg: TODO. -->
      </para>
    </listitem>
    </varlistentry>

    <varlistentry>
      <term>
        Problem: depending on which node the psiadmin command "status"
        is run, different group of nodes are seen as up or down.
      </term>
    <listitem>
      <para>
        Check for identical
        <filename>/etc/parastation.conf</filename>.
      </para>
    </listitem>
    </varlistentry>

    <varlistentry>
      <term>
        Warning: while starting up a parallel task the message
      </term>
    <listitem>
      <programlisting>
  execClient: chdir(/usr/tmp/username/./.): No such file or directory
  Will use user's home directory
  ---------------------------------------
      </programlisting>
      <para>
        appears.
      </para>
      <para>
        The current working directory
        <filename>/usr/tmp/username</filename> does not exist on one
        or more of the remote nodes. The users home directory, defined
        by <envar>HOME</envar> will be used instead.
      </para>
      <para>
        Make sure that the directory
        <filename>/usr/tmp/username</filename> is accessible on each
        node or change the working directory to a globally accessible
        directory.
      </para>
    </listitem>
    </varlistentry>

  </variablelist>
</chapter>
