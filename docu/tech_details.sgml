<chapter id="techdetails">
  <title>Insight &ps4;</title>

  <para>
    This chapter provides more technical details and background
    information about &ps4;. 
  </para>

  <section id="techdetails_psportlib">
<!--    MOD:  -->
    <title>&ps4; <filename>psport4</filename> communication library</title>
    <para>
      The &ps; communication library <filename>libpsport4</filename>
      offers secure and reliable end-to-end connections. Based on a
      port concept, it hides the actual transport and communication
      characteristics to the application and higher level libraries.
    </para>
    <para>
      The <filename>psport4</filename> library supports a wide range
      of interconnects and protocols for data transfers:
    </para>
    <itemizedlist>
      <listitem>
        <para>
          <filename>TCP:</filename> uses standard TCP/IP sockets to
          transfer data.
        </para>
      </listitem>
      <listitem>
        <para>
          <filename>P4sock:</filename> uses an optimized network
          protocol for Ethernet (see below).
        </para>
      </listitem>
      <listitem>
        <para>
          <filename>Infiniband:</filename> based on a vapi kernel
          layer, typically provided by the hardware vendor, the psport4
          library uses RDMA over Infiniband to actually transfer data.
        </para>
      </listitem>
      <listitem>
        <para>
          <filename>Myrinet:</filename> using the GM kernel level
          module, the psport4 library is able to use Myrinet for
          transfer data.
        </para>
      </listitem>
      <listitem>
        <para>
          <filename>Shared Memory:</filename> for communication within
          a SMP node, the psport library uses shared memory.
        </para>
      </listitem>
    </itemizedlist>
    <para>
      The interconnect and protocol used between two distinct
      processes is choosen while opening the connection between those
      processes. Depending on available hardware, configuration (see
      <xref linkend="config_ps"/>) and current environment variables
      (see <xref linkend="techdetails_compath"/>), the library
      automatically selects the fastest available communication
      method.
    </para>
    <para>
      The library routines for sending and receiving data handle
      arbitrary large buffers.  If necessary, the buffers will be
      fragmented and reassembled to meet the underlying transport
      requirements.
    </para>
  </section>

  <section id="techdetails_p4sock">
    <title>&ps4; protocol <emphasis>p4sock</emphasis></title>
    <para>
      &ps4; provides its own communication protocol for Ethernet,
      called <emphasis>p4sock</emphasis>.  This protocol is designed
      for extremely fast and reliable communication within a closed and
      homogeneous compute cluster environment. 
<!--
      <footnote>
        <para>
          Support for more interconnects is currently under
          development, especially Infiniband.
        </para>
      </footnote>
-->
    </para>
    <para>
<!--      MOD:  -->
      The protocol implements a reliable, connection-oriented
      communication layer, especially designed for very low overhead.
      Therefore, it delivers very low latencies.
    </para>
<!--
    <para>
      This protocol is only one communication path, supported by the
      &ps4; port library <emphasis>psport4.</emphasis> For more
      details refer to <xref linkend="techdetails_psportlib"/>.
    </para>
-->
    <para>
      The <emphasis>p4sock</emphasis> protocol is encapsulated within
      the kernel module <emphasis>p4sock.o</emphasis>. This module is
      loaded on system startup or whenever the &ps4; daemon <xref linkend="psid"/> starts
      up and the <emphasis>p4sock</emphasis> protocol is enabled
      within the configuration file <xref linkend="parastation_conf"/>.
    </para>
    <para>
      The <emphasis>p4sock.o</emphasis> module inserts a number of 
      entries within the <filename>/proc</filename> filesystem.  All
      &ps4; entries are located within the subdirectory
      <filename>/proc/sys/ps4</filename>. Three different
      subdirectories, listed below, are available.
    </para>
    <para>
      To read a value, eg. just type
    </para>
    <programlisting>  # cat /proc/sys/ps4/state/connections</programlisting>
    <para>
      to get the number of currently open connections. To modify a
      value, for eg. type
    </para>
    <programlisting>  # echo 10 > /proc/sys/ps4/state/ResendTimeout</programlisting>
    <para>
      to set the new value for <filename>ResendTimeout</filename>.
    </para>

    <section>
<!--    MOD:  -->
      <title>Directory <filename> /proc/sys/ps4/state</filename></title>
      <para>
        Within this <filename>state</filename> directory, various
        entries showing protocol counters are accessible. All this
        entries, except <filename>polling</filename>, are read only!
      </para>
      <para>
        <itemizedlist>
          <listitem>
            <para>
              <filename>HZ:</filename> reads the number of timer
              interrupts per second for this kernel ("jiffies").
            </para>
          </listitem>
          <listitem>
            <para>
              <filename>connections:</filename> reads the current number of open
              connections.
            </para>
          </listitem>
          <listitem>
            <para>
              <filename>polling:</filename> returns the current value
              for the polling flag: 0 = never poll, 1 = poll if
              otherwise idle (number of runable processes &lt; number
              of CPUs), 2 = always poll. Writing this value will
              immediately change the polling strategy.
            </para>
          </listitem>
          <listitem>
            <para>
              <filename>recv_net_ack:</filename> number of received
              ACKs.
            </para>
          </listitem>
          <listitem>
            <para>
              <filename>recv_net_ctrl:</filename> number of received
              control packets (ACK, NACK, SYN, SYNACK, ...).
            </para>
          </listitem>
          <listitem>
            <para>
              <filename>recv_net_data:</filename> number of received
              data packets.
            </para>
          </listitem>
          <listitem>
            <para>
              <filename>recv_net_nack:</filename> number of received
              NACKs. 
            </para>
          </listitem>
          <listitem>
            <para>
              <filename>recv_user:</filename> number of packets
              delivered to application buffers.
            </para>
          </listitem>
          <listitem>
            <para>
              <filename>send_net_ack:</filename> number of sent ACKs.
            </para>
          </listitem>
          <listitem>
            <para>
              <filename>send_net_ctrl:</filename> number of sent
              control packets.
            </para>
          </listitem>
          <listitem>
            <para>
              <filename>send_net_data:</filename> number of sent data
              packets.
            </para>
          </listitem>
          <listitem>
            <para>
              <filename>send_net_nack:</filename> number of sent
              NACKs.
            </para>
          </listitem>
          <listitem>
            <para>
              <filename>send_user:</filename> number of packets sent
              by the application.
            </para>
          </listitem>

          <listitem>
            <para>
              <filename>sockets:</filename> number of open sockets
              connecting to the &ps4; protocol module.
            </para>
          </listitem>
          <listitem>
            <para>
              <filename>timer_ack:</filename> number of expired
              delayed ACK timers.
            </para>
          </listitem>
          <listitem>
            <para>
              <filename>timer_resend:</filename> number of expired
              resend timers. 
            </para>
          </listitem>
        </itemizedlist>
<!--
        <itemizedlist>
          <listitem>
            <para>
              <filename>connections:</filename> reads the current number of open
              connections.
            </para>
          </listitem>
          <listitem>
            <para>
              <filename>recv_cnt:</filename> reads the number
              of packets received up to now.
            </para>
          </listitem>
          <listitem>
            <para>
              <filename>recv_oo_new_cnt:</filename> reports the number
              of "out of order" packets seen up to now, which were
              ahead of the expected packet sequence.
            </para>
          </listitem>
          <listitem>
            <para>
              <filename>recv_oo_new_double_cnt:</filename> reads the
              number of "out of order" packets seen up to now, which
              were ahead of the expected packet sequence and have been
              already received.
            </para>
          </listitem>
          <listitem>
            <para>
              <filename>recv_oo_old_cnt:</filename> reads the number
              of "out of order" packets behind the current sequence
              (already received).
            </para>
          </listitem>
          <listitem>
            <para>
              <filename>resend_cnt:</filename> reports the number of
              re-sents up to now. Each re-send can transmit one or
              more packets. Re-sends are initiated by timeouts or NACK
              messages.
            </para>
          </listitem>
          <listitem>
            <para>
              <filename>resend_timer_cnt:</filename> reports the
              number of re-sends due to ACK/NACK timeouts. Each
              incrementation indicates one or more resent packages.
            </para>
          </listitem>
          <listitem>
            <para>
              <filename>send_cnt:</filename> reports the number of
              packets sent up to now.
            </para>
          </listitem>
          <listitem>
            <para>
              <filename>send_timed_ack_cnt:</filename> reports the
              number of delayed ACK messages sent up to now.
            </para>
          </listitem>
          <listitem>
            <para>
              <filename>send_urgent_ack_cnt:</filename> reports the
              number of "urgent" ACK messages sent.
            </para>
          </listitem>
          <listitem>
            <para>
              <filename>sockets:</filename> number of open sockets
              connecting to the &ps4; protocol module.
            </para>
          </listitem>
        </itemizedlist>
-->
      </para>
    </section>

    <section>
      <title>Directory <filename>/proc/sys/ps4/ether</filename></title>
      <para>
        Within this directory, all Ethernet related parameters for the
        &ps4; <emphasis>p4sock</emphasis> protocol are grouped. All
        this entries can be read and written, newly written values will be
        used immediately.
      </para>
      <para>
        <itemizedlist>
          <listitem>
            <para>
              <filename>AckDelay:</filename> maximum delay in "jiffies"
                <footnote>
                <para>
                  A jiffy is the base unit for system timers, used by
                  the Linux kernel. So all timeouts within the kernel
                  are based on this timer resolution. On kernels with
                  version 2.4, this it typically 100Hz (= 10ms). But
                  there are kernel versions available, eg. for newer
                  SuSE Linux versions, which include patches to change
                  this to a much higher value! 
                </para>
                <para>
                  The resolution of the timer interrupt can be
                  evaluated by reading
                  <filename>/proc/sys/ps4/state/HZ</filename>.
                </para>
                </footnote>
              for ACK messages. If no message is sent within this time
              frame, where an ACK for already received packets can be
              "hooked up", a single ACK message will generated. 
              <!-- Otherwise, ACKs are "hooked up" to regular data packets.  -->
              Must be less then <filename>ResendTimeout</filename>.
            </para>
          </listitem>
          <listitem>
              <para>
              <filename>MaxAcksPending:</filename> maximum number of
              pending ACK messages until an "urgent" ACK messages will
              be sent.
            </para>
          </listitem>
          <listitem>
            <para>
              <filename>MaxDevSendQSize:</filename> maximum number of
              entries of the (protocol internal) send queue to the
              network device.
            </para>
          </listitem>
          <listitem>
            <para>
              <filename>MaxMTU:</filename> maximum packet size used
              for network packets. For sending packets, the minimum of
              <filename>MaxMTU</filename> and service specific MTU
              will be used.
            </para>
          </listitem>
          <listitem>
            <para>
              <filename>MaxRecvQSize:</filename> size of the protocol
              internal receive queue.
            </para>
          </listitem>
          <listitem>
            <para>
              <filename>MaxResend:</filename> Number of retries until
              a connection is declared as dead.
            </para>
          </listitem>
          <listitem>
            <para>
              <filename>MaxSendQSize:</filename> size of the protocol
              internal send queue.
            </para>
          </listitem>
          <listitem>
            <para>
              <filename>ResendTimeout:</filename> delay in "jiffies"
              for resending packets not acknowledged up to now. Must
              be greater then <filename>AckDelay</filename>.
          </para>
          </listitem>
        </itemizedlist>
      </para>
    </section>

    <section>
      <title>Directory <filename>/proc/sys/ps4/local</filename></title>
      <para>
        Currently, there are no entries defined for this directory.
      </para>
    </section>

    <section>
      <title><filename>p4stat</filename></title>
      <para>
        The command <filename>p4stat</filename> can be used to list
        open sockets and network connections of the
        <emphasis>p4sock</emphasis> protocol.
        <programlisting width="80">
  $ /opt/parastation/bin/p4stat -s
  Socket #0 : Addr: &lt;00&gt;&lt;00&gt;&lt;00&gt;&lt;00&gt;&lt;00&gt;&lt;00&gt;&lt;00&gt;&lt;00&gt;'........' last_idx 0 refs 2
  Socket #1 : Addr: &lt;70&gt;&lt;6f&gt;&lt;72&gt;&lt;74&gt;&lt;33&gt;&lt;38&gt;&lt;34&gt;&lt;00&gt;'port384.' last_idx 0 refs 10
  Socket #2 : Addr: &lt;70&gt;&lt;6f&gt;&lt;72&gt;&lt;74&gt;&lt;31&gt;&lt;34&gt;&lt;34&gt;&lt;00&gt;'port144.' last_idx 0 refs 10
  
  $ /opt/parastation/bin/p4stat -n
  net_idx      SSeqNo SWindow RSeqNo RWindow lusridx lnetidx rnetidx snq rnq refs
       84       30107   30467  30109   30468      84      84     230   0 0    2
       85       30106   30466  30106   30465      85      85     231   0 0    2
       86       30107   30467  30109   30468      86      86      84   0 0    2
       87       30106   30466  30106   30465      87      87      85   0 0    2
       88       30107   30467  30109   30468      88      88     217   0 0    2
       89       30106   30466  30106   30465      89      89     218   0 0    2
       90       30106   30466  30106   30465      90      90     220   0 0    2
       91       30106   30466  30106   30465      91      91     221   0 0    2
       92       30001   30361  30003   30362      92      92     232   0 0    2
       93       30001   30361  30003   30362      93      93     219   0 0    2
       94       30000   30000  30001   30360      94      94     233   0 0    2
       95       30000   30000  30001   30360      95      95     222   0 0    2
       96       30000   30000  30001   30360      96      96     222   0 0    2
       97       30000   30000  30001   30360      97      97     227   0 0    2
       98       30000   30000  30001   30360      98      98      86   0 0    2
       99       30001   30361  30003   30362      99      99      93   0 0    2
          </programlisting>
        This command shows some protocol internal parameters (open
        connections, sequence numbers, etc.).
      </para>
      <para>
        For more information, see <xref linkend="p4stat"/>.
      </para>
    </section>
  </section>

<!-- ============== Process distribution ============== -->
  <section id="techdetails_process_distribution">
    <title>Controlling process placement</title>
    <para>
      &ps; includes sophisticated functions to control the process
      placement for newly created parallel tasks.
      Upon task startup, the environment
      variables
      <parameter>PSI_NODES</parameter>,
      <parameter>PSI_HOSTS</parameter> and
      <parameter>PSI_HOSTFILE</parameter> are looked up (in this
      order) to get a predefined node list. If not defined, all
      currently known nodes are taken into account. Also, the variables
      <parameter>PSI_NODES_SORT</parameter>,
      <parameter>PSI_LOOP_NODES_FIRST</parameter> and
      <parameter>PSI_EXCLUSIVE</parameter>
      are observed. Based on this variables, a sorted list of nodes is
      constructed, defining the final node list for this new task.
    </para>
    <para>
      Beside this environment variables, node reservations for users
      and groups are also observed. See <xref linkend="psiadmin"/>. 
    </para>
    <para>
      In addition, only available nodes will be used to start up
      processes. Currently not available nodes will be ignored and
      automatically replaced by other nodes, if available.
    </para>
    <para>
      If no environment variables controlling the process placement
      are defined, the processes will be spawned on nodes having the
      lowest load. For SMP systems, all available CPUs on this node
      will be used for consecutive ranks.
    </para>
    <para>
      For a detailed discussion of placing processes within &ps4;,
      please refer to 
      <citerefentry>
        <refentrytitle>process placement</refentrytitle>
        <manvolnum>7</manvolnum> 
      </citerefentry>
      and
      <citerefentry>
        <refentrytitle>ps_environment</refentrytitle>
        <manvolnum>5</manvolnum> 
      </citerefentry>.
    </para>
  </section>

<!-- ============== Queuing ============== -->
  <section id="techdetails_queuing">
    <title>Using the &ps4; queuing facility</title>
    <para>
      Beginning with version 4.1, &ps; is able to queue task start
      request if required resources are currently unavailable. This
      queuing facility is disabled by default. It can be enabled by
      each user independently. All requests of all users are held
      within one queue and managed on a first-come-first-serve
      strategy. 
    </para>
    <para>
      For details, refer to &psug;.
    </para>
  </section>

<!-- ============== Environment ============== -->
  <section id="techdetails_environment">
    <title>Exporting environment variables for a task</title>
    <para>
      &ps; by default exports only a limited set of environment
      variables to newly spawned processes. These variables are:
        <parameter>HOME</parameter>,
        <parameter>USER</parameter>,
        <parameter>SHELL</parameter> and
        <parameter>TERM</parameter>.
    </para>
    <para>
      Additional variables can be exported using
      <envar>PSI_EXPORTS</envar>.
    </para>
    <para>
      For a complete list of environment variables known to &ps;,
      refer to
      <citerefentry>
        <refentrytitle>ps_environment</refentrytitle>
        <manvolnum>5</manvolnum> 
      </citerefentry> .
    </para>
    <para>
      For more details, refer to &psug;.
    </para>
  </section>

<!-- ============== mpirun_chp4, mpirun_chgm ============== -->
  <section id="techdetails_chp4">
    <title>Using non-&ps; applications</title>
    <para>
      It is possible to run programs linked with 3rd party MPI libraries
      within the &ps; environment. Currently supported MPI libraries are:
    </para>
    <itemizedlist>
      <listitem>
        <para> <parameter>MPIch using ch_p4</parameter> </para>
      </listitem>
      <listitem>
        <para> <parameter>MPIch using GM</parameter> </para>
      </listitem>
    </itemizedlist>
    <para>
      In order to run applications linked with this libraries, &ps4;
      provides dedicated <command>mpirun</command> commands. The
      processes for this parallel tasks are spawned obeying all restrictions
      described in <xref linkend="techdetails_process_distribution"/>.
      Of course, the data transfer will be based on the communication channels
      supported by this MPI library. For MPIch using chp4 (TCP), &ps4; provides an
      alternative, see <xref
      linkend="techdetails_tcp_bypass"/>.
    </para>
    <para>
      For more information refer to 
      <citerefentry>
        <refentrytitle>mpirun_chp4</refentrytitle>
        <manvolnum>8</manvolnum> 
      </citerefentry>
      and 
      <citerefentry>
        <refentrytitle>mpirun_chgm</refentrytitle>
        <manvolnum>8</manvolnum> 
      </citerefentry>
      .
    </para>
    <para>
      It is also possible to run serial applications, thus
      applications not parallelized with MPI, within &ps;. To execute
      a program, run <filename>psistart</filename>. For more details
      refer to   
      <citerefentry>
        <refentrytitle>psistart</refentrytitle>
        <manvolnum>8</manvolnum> 
      </citerefentry>
      and &psug;.
    </para>
  </section>

  <!-- ============== TCP Bypass ============== -->
  <section id="techdetails_tcp_bypass">
    <title>&ps4; TCP bypass</title>
    <para>
      Beginning with version 4.1, &ps4; offers a feature called "TCP
      bypass", enabling applications based on TCP to use the efficient
      <emphasis>p4sock</emphasis> protocol.  The data will be
      redirected within the kernel to the
      <emphasis>p4sock</emphasis>
      protocol. No modifications to the application are necessary!
    </para>
    <para>
      To automatically enable the TCP bypass, insert a line like
    </para>
    <programlisting>
  env PS_TCP &lt;FirstAddress&gt;-&lt;LastAddress&gt;
    </programlisting>
    <para>
      into the configuration file
      <filename>parastation.conf</filename>, were
      <parameter>FirstAddress</parameter> and
      <parameter>LastAddress</parameter> are the first and last IP
      addresses for which the bypass should be enabled.
    </para>
    <note>
      <para>
      Daemons opening TCP sockets and listening for connections, eg.
      <command>inetd</command>, must be restarted after loading and
      configuring the TCP bypass module.
      </para>
    </note>
    <para>
      For more information, see <xref linkend="p4tcp"/>.
    </para>
  </section>

  <!-- ============== Controlling communication paths ============== -->
  <section id="techdetails_compath">
    <title>Controlling &ps4; communication paths</title>
    <para>
      &ps; is able to use different communication paths in parallel,
      see <xref linkend="techdetails_psportlib"/>. In order to
      restrict the paths to use, a number of environment variables are
      recognized by &ps;.
    </para>
    <variablelist>
      <varlistentry>
        <term>
          <envar>PSP_SHAREDMEM</envar>
        </term>
        <listitem>
          <para>
            Don't use shared memory for communication within the same
            node.
          </para>
        </listitem>
      </varlistentry>
<!-- fg: why not implemented?
      <varlistentry>
        <term>
          <envar>PSP_TCP</envar>
        </term>
        <listitem>
          <para>
            Don't use TCP for communication.
          </para>
        </listitem>
      </varlistentry>
-->
      <varlistentry>
        <term>
          <envar>PSP_P4SOCK</envar>
        </term>
        <listitem>
          <para>
            Don't use &ps; p4sock protocol for communication.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>
          <envar>PSP_MVAPI</envar>
        </term>
        <listitem>
          <para>
<!--            MOD:  -->
            Don't use Infiniband vapi for communication.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>
          <envar>PSP_GM</envar>
        </term>
        <listitem>
          <para>
<!--            MOD:  -->
            Don't use GM (Myrinet) for communication.
          </para>
        </listitem>
      </varlistentry>
    </variablelist>
    <para>
      To disable the particular transport, the corresponding variable
      must be set to 0, to enable a transport, the variable must be
      set to 1 or the variable must not be defined.
    </para>
    <para>
<!--      MOD:  -->
      It is not possible to dynamically disable TCP as a
      communication path. TCP, if configured, is always used as a
      last resort for communication.
    </para>
  </section>
  <!-- ============== Authentification ============== -->
  <section id="techdetails_auth">
    <title>Authentification within &ps4;</title>
    <para>
      Whenever a process of a parallel task is spawned within the
      cluster, &ps; does not authenticate the user. Only the user and
      group id is copied to the remote node and used for starting up
      processes. 
    </para>
    <para>
      Thus, it is not necessary for the user to be known by the
      compute node, e.g. having an entry in
      <filename>/etc/passwd</filename>. On the contrary, the
      administrator may disallow logins for users by removing the
      entries from <filename>/etc/passwd</filename>. Usage of common
      authentification schemes like <filename>NIS</filename>
      is not required and therefore limits user management to the
      frontend nodes.
    </para>
    <para>
      Authentification of users is restricted to login or frontend
      nodes and is outside of the scope of &ps;.
    </para>
  </section>
  <!-- ============== Homogeneous user ID space ============== -->
  <section id="techdetails_userspace">
    <title>Homogeneous user ID space </title>
    <para>
      As explained in the previous section, &ps; uses only user and
      group IDs for starting up remote processes. Therefore, all
      processes will have identical user and group IDs on all nodes.
    </para>
    <para>
      A homogeneous user ID space is stretched across the entire
      cluster. 
    </para>
  </section>
  <!-- ============== Single system view ============== -->
  <section id="techdetails_singlesystemview">
    <title>Single system view</title>
    <para>
      The &ps; management tools present information of all nodes, or a
      subset of nodes, if requested, at one place. Also
      commands can be launched on each node and the actions initiated
      by the user or administrator are automatically transfered to the
      destination node(s). Thus, the administrator will have a
      <emphasis>single system view</emphasis> of the cluster.
      <!-- TODO: fg: describe single system view aspects -->
    </para>
  </section>
  <!-- ============== parallel shell tool ============== -->
  <section id="techdetails_parashell">
    <title>Parallel shell tool</title>
    <para>
      Beginning with &ps; version 4.1, a parallel shell tool called
      <filename>psh</filename> will be part of &ps;. This tool allows
      to run commands on all or selected nodes of the cluster in
      parallel. It also presents the output of the commands in a
      sophisticated manner, showing common parts and differences.
      <!-- TODO: fg: describe -->
    </para>
    <para>
      This command is intended not to run interactive commands in
      parallel, but to run a single task in parallel on all or a bunch
      of nodes and prepare the output to be easily read by the user.
    </para>
    <para>
      For more details see 
      <citerefentry>
        <refentrytitle>psh</refentrytitle>
        <manvolnum>8</manvolnum> 
      </citerefentry>.
    </para>
  </section>
  <!-- ============== nodes and cpus ============== -->
  <section id="techdetails_nodescpus">
    <title>Nodes and CPUs</title>
    <!-- TODO: fg: describe more precisely -->
    <para>
<!--      MOD:  -->
      Though &ps; by default tries to use a dedicated CPU per compute
      process, there is currently no way to bind a process to a
      particular CPU. Therefore, there is no guarantee,
      that each process will use its own CPU. But due to the nature
      of parallel tasks, the operating system scheduler will
      typically distribute each process to its own CPU.
    </para>
    <para>
      Care must be taken if the hardware is able to simulate virtual
      CPUs, eg. Intel Xeon CPUs using Hyperthreading. This is
      currently not detected by &ps; and may lead to "overbook" the
      number of physical CPUs with processes. Use
    </para>
    <programlisting>  psiadmin -c "s hw"</programlisting>
    <para>
      to show the number of available (logical) CPUs per node.
    </para>
    <para>
<!--      MOD:  -->
      Please note, it's possible to spawn more processes than CPUs
      are available on a node ("overbooking"). See &psug; for
      details.
    </para>
  </section>
  <!-- ============== AFS ============== -->
  <section id="techdetails_AFS">
    <title>Integration with AFS</title>
    <para>
      To run parallel tasks spawned by &ps; on clusters using
      <filename>AFS</filename>, &ps; provides the scripts
      <filename>env2tok</filename> and <filename>tok2env</filename>.
    </para>
    <para>
      On the frontend side, calling
    </para>
    <programlisting> . gettoken</programlisting>
    <para>
      will create an environment variable
      <filename>AFS_TOKEN</filename> containing an encoded access
      token for AFS. This variable must be added to the list of
      exported variables
    </para>
    <programlisting>  PSI_EXPORTS="AFS_TOKEN,$PSI_EXPORTS"</programlisting>
    <para>
      In addition, the variable
    </para>
    <programlisting>  PSI_RARG_PRE_0=/some/path/env2tok</programlisting>
    <para>
      must be set. This will call the script
      <filename>env2tok</filename> before running the actual program
      on each node. <filename>Env2tok</filename> itself will decode
      the token and will setup the AFS environment.
    </para>
    <note>
      <para>
        The commands <filename>SetToken</filename> and
        <filename>GetToken</filename>, which are part of the AFS
        package, must be available on each node.  Also, the commands
        <filename>uuencode</filename> and
        <filename>uudecode</filename> must be installed.
      </para>
    </note>
    <para>Script <filename>env2tok</filename>:</para>
    <programlisting>
  #!/bin/bash
  tmp=$IFS
  IFS=" "
  export AFS_TOKEN=`GetToken | uuencode /dev/stdout`
  IFS=$tmp
    </programlisting>
    <para>Script <filename>tok2env</filename>:</para>
    <programlisting>
  #!/bin/bash
  IFS=" "
  echo $AFS_TOKEN | uudecode | SetToken

  exec $*
    </programlisting>
  </section>

  <!-- ============== batch systems ============== -->
  <section id="techdetails_batchsystems">
    <title>Integrating external queuing systems</title>
    <para>
      &ps; can be easily integrated with batch queuing  and scheduling
      systems.  In this case, the queuing system will decide, where
      (and when) to run a parallel task. &ps; will then start, monitor
      and terminate the task. In case of higher priorized jobs, the
      batch system may also suspend a task using the &ps; signal
      forwarding.
    </para>
    <para>
      Integration is done by setting up &ps; environment variables,
      like <envar>PSI_HOSTFILE</envar>. 
      &ps; itself need not be modified in any way.
      It is not necessary to use a
      remote shell (<filename>rsh</filename>) to start
      <filename>mpirun</filename> on the first node of the selected
      partition. The batch system should only run the command on the
      same node where the batch system is running, &ps; will start all
      necessary processes on the remote nodes. For details about
      spawning processes refer to &psug;.
    </para>
    <note>
      <para>
        If an external queuing system is used, the environment
        variable <envar>PSI_NODES_SORT</envar> should be set to
        "none", thus no sorting of any predefined node list will be
        done by &ps;.
      </para>
    </note>
    <para>
      Beginning with version 4.1, &ps; includes its own queuing
      facility. For more details, refer to <xref
      linkend="techdetails_queuing"/> and &psug;.
    </para>

    <section id="config_integrationPBSPro">
      <title>Integration with PBS&nbsp;PRO</title>
      <para>
        Parallel jobs started by PBS&nbsp;PRO using the &ps;
        <filename>mpirun</filename> command
        will be automatically recognized. Due to the environment
        variable <envar>PBS_NODEFILE</envar>, defined by PBS&nbsp;PRO, &ps;
        will automatically setup the <envar>PSI_HOSTFILE</envar> to
        <envar>PBS_NODEFILE</envar>.
      </para>
      <para>
        Therefore, &ps; will use the (unsorted) hostfile supplied by
        PBS&nbsp;PRO to startup the parallel task.
      </para>
    </section>
    <section id="config_integrationOpenPBS">
      <title>Integration with OpenPBS</title>
      <para>
        Refer to previous <xref linkend="config_integrationPBSPro"/>.
      </para>
    </section>
    <section id="config_integrationLSF">
      <title>Integration with LSF</title>
      <para>
        Similar to <xref
        linkend="config_integrationPBSPro"/>, &ps; will also recognize
        the variable <envar>LSB_HOSTS</envar>, provided by LSF.
        This variable holds a list of nodes for the parallel task. It
        is copied to the &ps; variable <envar>PSI_HOSTS</envar>,
        consequently it will be used for starting up the task. 
<!--
        In
        addition, the variable <envar>PSI_NODES_SORT</envar> will be
        set to "none", thus no sorting to this list will occur.
-->
      </para>
    </section>
  </section>

  <!-- ============== spare nodes ============== -->
  <section id="techdetails_spareNodes">
    <title>Integrating spare nodes</title>
    <!-- TODO: fg: describe more precisely -->
    <!-- TODO: fg: does not apply if a node list is provided! -->
    <para>
      &ps; is able and will in fact automatically use stand by or
      spare nodes, provided that this nodes are installed and
      connected to the cluster. 
      This kind of nodes can be added to the cluster
      like any regular compute node. &ps; will use this nodes like
      any other node. Consequently, this node type will be not only
      used in "hot standby", in fact it is a real part of the
      cluster. 
    </para>
    <para>
      In case one of the nodes fails, this node will be ignored for
      future tasks without administrator intervention due to the
      load balancing and node monitoring facilities of &ps;. 
    </para>
    <para>
      As a consequence, spare nodes will not only increase the
      overall availablity, but can also be used to increase the
      compute power of the entire cluster.
<!--
      <emphasis>Run with (100+x)% compute
      power and in case of a failure, still 100% are left.</emphasis>
-->
    </para>
    <note>
      <para>
<!--        MOD:  -->
        If a batch system is used which controls the placement of
        processes by itself, this automatic usage of spare nodes will
        not take place unless enforced by the batch system.
      </para>
    </note>
  </section>
</chapter>

  <!-- Keep this comment at the end of the file
  Local variables:
  mode: xml
  sgml-omittag:nil
  sgml-shorttag:nil
  sgml-namecase-general:nil
  sgml-general-insert-case:lower
  sgml-minimize-attributes:nil
  sgml-always-quote-attributes:t
  sgml-indent-step:2
  sgml-indent-data:t
  sgml-parent-document:("userguide.xml" "book" "book" ("title" "bookinfo"))
  sgml-exposed-tags:nil
  sgml-local-catalogs:nil
  sgml-local-ecat-files:nil
  End:
  -->
