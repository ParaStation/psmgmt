<chapter id="techdetails">
  <title>Insight &ps4;</title>

  <para>
    This chapter provides more technical details and background
    information about &ps4;. 
  </para>

  <section id="techdetails_psportlib">
    <title>&ps4; <filename>psport4</filename> communication library</title>
    <para>
      TODO.
    </para>
  </section>

  <section id="techdetails_p4sock">
    <title>&ps4; protocol <emphasis>p4sock</emphasis></title>
    <para>
      &ps4; provides its own kernel level communication protocol,
      called <emphasis>p4sock</emphasis>.  This protcol is designed
      for extremly fast and reliable communication within a closed and
      homogeneous compute cluster environment. Currently, all kinds of
      Ethernet based interconnects are supported.
      <footnote>
        <para>
          Support for more interconnects is currently under
          development, especially InfiniBand.
        </para>
      </footnote>
    </para>
    <para>
      This protocol is only one communication path, supported by the
      &ps4; port library <emphasis>psport4.</emphasis> For more
      details refer to <xref linkend="techdetails_psportlib"/>.
    </para>
    <para>
      The <emphasis>p4sock</emphasis> protocol is encapsulated within
      the kernel module <emphasis>p4sock.o</emphasis>. This module is
      loaded whenever the &ps4; daemon <xref linkend="psid"/> starts
      up and the <emphasis>p4sock</emphasis> protocol is enabled
      within the configuration file <xref linkend="parastation_conf"/>.
    </para>
    <para>
      The <emphasis>p4sock.o</emphasis> module inserts a number of new
      entries within the <filename>/proc</filename> filesystem.  All
      &ps4; entries are located within the subdirectory
      <filename>/proc/sys/ps4</filename>. Three different
      subdirectories listed below are available.
    </para>
    <para>
      To read a value, eg. just type
    </para>
    <programlisting>  # cat /proc/sys/ps4/state/connections</programlisting>
    <para>
      to get the number of currently open connections. To modify a
      value, for eg. type
    </para>
    <programlisting>  # echo 10 > /proc/sys/ps4/state/ResendTimeout</programlisting>
    <para>
      to set the new value for <filename>ResendTimeout</filename>.
    </para>

    <section>
      <title>directory <filename>/proc/sys/ps4/state</filename></title>
      <para>
        Within this <filename>state</filename> directory, various
        entries showing protocol counters are accessible. All this
        entries are read only!
      </para>
      <para>
        <itemizedlist>
          <listitem>
            <para>
              <filename>connections:</filename> reads the current number of open
              connections.
            </para>
          </listitem>
          <listitem>
            <para>
              <filename>recv_cnt:</filename> reads the number
              of packets received up to now.
            </para>
          </listitem>
          <listitem>
            <para>
              <filename>recv_oo_new_cnt:</filename> reports the number
              of "out of order" packets seen up to now, which were
              ahead of the expected packet sequence.
            </para>
          </listitem>
          <listitem>
            <para>
              <filename>recv_oo_new_double_cnt:</filename> reads the
              number of "out of order" packets seen up to now, which
              were ahead of the expected packet sequence and have been
              already received.
            </para>
          </listitem>
          <listitem>
            <para>
              <filename>recv_oo_old_cnt:</filename> reads the number
              of "out of order" packets behind the current sequence
              (already received).
            </para>
          </listitem>
          <listitem>
            <para>
              <filename>resend_cnt:</filename> reports the number of
              re-sents up to now. Each re-send can transmit one or
              more packets. Re-sends are initiated by timeouts or NACK
              messages.
            </para>
          </listitem>
          <listitem>
            <para>
              <filename>resend_timer_cnt:</filename> reports the
              number of re-sends due to ACK/NACK timeouts. Each
              incrementation indicates one or more resent packages.
            </para>
          </listitem>
          <listitem>
            <para>
              <filename>send_cnt:</filename> reports the number of
              packets sent up to now.
            </para>
          </listitem>
          <listitem>
            <para>
              <filename>send_timed_ack_cnt:</filename> reports the
              number of delayed ACK messages sent up to now.
            </para>
          </listitem>
          <listitem>
            <para>
              <filename>send_urgent_ack_cnt:</filename> reports the
              number of "urgent" ACK messages sent.
            </para>
          </listitem>
          <listitem>
            <para>
              <filename>sockets:</filename> number of open sockets
              connecting to the &ps4; protocol module.
            </para>
          </listitem>
        </itemizedlist>
      </para>
    </section>

    <section>
      <title>directory <filename>/proc/sys/ps4/ether</filename></title>
      <para>
        Within this directory, all Ethernet related counters and
        values for the &ps4; <emphasis>p4sock</emphasis> protocol are
        grouped. All this entries can be read and written, the
        new values will be used immediately.
      </para>
      <para>
        <itemizedlist>
          <listitem>
            <para>
              <filename>AckDelay:</filename> maximum delay in "jiffies"
                <footnote>
                <para>
                  A jiffy is the base unit for system timers, used by
                  the Linux kernel. So all timeouts within the kernel
                  are based on this timer resolution. On kernels with
                  version 2.4, this it typically 100Hz (= 10ms). But
                  there are kernel versions available, eg. for newer
                  SuSE Linux versions, which include patches to change
                  this to a much higher value! 
                </para>
                </footnote>
              for ACK messages. If no message is sent within this time
              frame, where an ACK for already received packets can be
              "hooked up", a single ACK message will generated. 
              <!-- Otherwise, ACKs are "hooked up" to regular data packets.  -->
              Must be less then <filename>ResendTimeout</filename>.
            </para>
          </listitem>
          <listitem>
              <para>
              <filename>MaxAcksPending:</filename> maximum number of
              pending ACK messages until an "urgent" ACK messages will
              be sent.
            </para>
          </listitem>
          <listitem>
            <para>
              <filename>MaxDevSendQSize:</filename> maximum number of
              entries of the (protocol internal) send queue to the
              network device.
            </para>
          </listitem>
          <listitem>
            <para>
              <filename>MaxMTU:</filename> maximum packet size used
              for network packets. For sending packets, the minimum of
              <filename>MaxMTU</filename> und service specific MTU
              will be used.
            </para>
          </listitem>
          <listitem>
            <para>
              <filename>MaxRecvQSize:</filename> size of the protocol
              internal receive queue.
            </para>
          </listitem>
          <listitem>
            <para>
              <filename>MaxResend:</filename> Number of retries until
              a connection is declared as dead.
            </para>
          </listitem>
          <listitem>
            <para>
              <filename>MaxSendQSize:</filename> size of the protocol
              internal send queue.
            </para>
          </listitem>
          <listitem>
            <para>
              <filename>ResendTimeout:</filename> delay in "jiffies"
              for resending packets not acknowledged up to now. Must
              be greater then <filename>AckDelay</filename>.
          </para>
          </listitem>
        </itemizedlist>
      </para>
    </section>

    <section>
      <title>directory <filename>/proc/sys/ps4/local</filename></title>
      <para>
        Currently, there are no entries defined for this directory.
      </para>
    </section>

    <section>
      <title><filename>p4stat</filename></title>
      <para>
        The command <filename>p4stat</filename> can be used to list
        open sockets and network connections of the
        <emphasis>p4sock</emphasis> protocol.
        <programlisting width="80">
  $ /opt/parastation/bin/p4stat -s
  Socket #0 : Addr: &lt;00&gt;&lt;00&gt;&lt;00&gt;&lt;00&gt;&lt;00&gt;&lt;00&gt;&lt;00&gt;&lt;00&gt;'........' last_idx 0 refs 2
  Socket #1 : Addr: &lt;70&gt;&lt;6f&gt;&lt;72&gt;&lt;74&gt;&lt;33&gt;&lt;38&gt;&lt;34&gt;&lt;00&gt;'port384.' last_idx 0 refs 10
  Socket #2 : Addr: &lt;70&gt;&lt;6f&gt;&lt;72&gt;&lt;74&gt;&lt;31&gt;&lt;34&gt;&lt;34&gt;&lt;00&gt;'port144.' last_idx 0 refs 10
  
  $ /opt/parastation/bin/p4stat -n
  net_idx      SSeqNo SWindow RSeqNo RWindow lusridx lnetidx rnetidx snq rnq refs
       84       30107   30467  30109   30468      84      84     230   0 0    2
       85       30106   30466  30106   30465      85      85     231   0 0    2
       86       30107   30467  30109   30468      86      86      84   0 0    2
       87       30106   30466  30106   30465      87      87      85   0 0    2
       88       30107   30467  30109   30468      88      88     217   0 0    2
       89       30106   30466  30106   30465      89      89     218   0 0    2
       90       30106   30466  30106   30465      90      90     220   0 0    2
       91       30106   30466  30106   30465      91      91     221   0 0    2
       92       30001   30361  30003   30362      92      92     232   0 0    2
       93       30001   30361  30003   30362      93      93     219   0 0    2
       94       30000   30000  30001   30360      94      94     233   0 0    2
       95       30000   30000  30001   30360      95      95     222   0 0    2
       96       30000   30000  30001   30360      96      96     222   0 0    2
       97       30000   30000  30001   30360      97      97     227   0 0    2
       98       30000   30000  30001   30360      98      98      86   0 0    2
       99       30001   30361  30003   30362      99      99      93   0 0    2
          </programlisting>
        This command shows some protocol internal parameters (open
        connections, sequence numbers, etc.).
      </para>
      <para>
        For more information, see <xref linkend="p4stat"/>.
      </para>
    </section>
  </section>

<!-- ============== Process distribution ============== -->
  <section id="techdetails_process_distribution">
    <title>Controlling process placement</title>
    <para>
      &ps; includes sophisticated functions to control the process
      placement for newly created parallel tasks.
      Upon task startup, the environment
      variables
      <parameter>PSI_NODES</parameter>,
      <parameter>PSI_HOSTS</parameter> and
      <parameter>PSI_HOSTFILE</parameter> are looked up (in this
      order) to get a predefined node list. If not defined, all
      currently known nodes are taken into account. Also, the variables
      <parameter>PSI_NODES_SORT</parameter>,
      <parameter>PSI_LOOP_NODES_FIRST</parameter> and
      <parameter>PSI_EXCLUSIVE</parameter>
      are observed. Based on this variables, a sorted list of nodes is
      constructed, defining the final node list for this new task.
    </para>
    <para>
      Beside this environment variables, node reservations for users
      and groups are also observed. See <xref linkend="psiadmin"/>. 
    </para>
    <para>
      In addition, only available nodes will be used to start up
      processes. Currently not available nodes will be ignored and
      automatically replaced by other nodes, if available.
    </para>
    <para>
      If no environment variables controlling the process placement
      are defined, the processes will be spawned on nodes having the
      lowest load. For SMP systems, all available CPUs on this node
      will be used for consecutive ranks.
    </para>
    <para>
      For a detailed discussion of placing processes within &ps4;,
      please refer to 
      <citerefentry>
        <refentrytitle>process placement</refentrytitle>
        <manvolnum>7</manvolnum> 
      </citerefentry>
      and
      <citerefentry>
        <refentrytitle>ps_environment</refentrytitle>
        <manvolnum>5</manvolnum> 
      </citerefentry>.
    </para>
  </section>

<!-- ============== Queuing ============== -->
  <section id="techdetails_queuing">
    <title>Using the &ps4; queuing facility</title>
    <para>
      Beginning with version 4.1, &ps; is able to queue task start
      request if required resources are currently unavailable. This
      queuing facility is disabled by default. It can be enabled by
      each user independently. All requests of all users are held
      within one queue and managed on a first-come-first-serve
      strategy. 
    </para>
    <para>
      For details, refer to &psug;.
    </para>
  </section>

<!-- ============== Environment ============== -->
  <section id="techdetails_environment">
    <title>Exporting environment variables for a task</title>
    <para>
      &ps; by default exports only a limited set of environment
      variables to newly spawned processes. These variables are:
        <parameter>HOME</parameter>,
        <parameter>USER</parameter>,
        <parameter>SHELL</parameter> and
        <parameter>TERM</parameter>.
    </para>
    <para>
      Additional variables can be exported using
      <envar>PSI_EXPORTS</envar>.
    </para>
    <para>
      For a complete list of environment variables known to &ps;,
      refer to
      <citerefentry>
        <refentrytitle>ps_environment</refentrytitle>
        <manvolnum>5</manvolnum> 
      </citerefentry> .
    </para>
    <para>
      For more details, refer to &psug;.
    </para>
  </section>

<!-- ============== mpirun_chp4, mpirun_chgm ============== -->
  <section id="techdetails_chp4">
    <title>Using non-&ps; applications</title>
    <para>
      It is possible to run programs linked with 3rd party MPI libraries
      within the &ps; environment. Currently supported MPI libraries are:
    </para>
    <itemizedlist>
      <listitem>
        <para> <parameter>MPIch using ch_p4</parameter> </para>
      </listitem>
      <listitem>
        <para> <parameter>MPIch using GM</parameter> </para>
      </listitem>
    </itemizedlist>
    <para>
      In order to run applications linked with this libraries, &ps4;
      provides dedicated <command>mpirun</command> commands. The
      processes for this parallel tasks are spawned obeying all restrictions
      described in <xref linkend="techdetails_process_distribution"/>.
      Of course, the data transfer will be based on the communication channels
      supported by this MPI library. For MPIch using chp4 (TCP), &ps4; provides an
      alternative, see <xref
      linkend="techdetails_tcp_bypass"/>.
    </para>
    <para>
      For more information refer to 
      <citerefentry>
        <refentrytitle>mpirun_chp4</refentrytitle>
        <manvolnum>8</manvolnum> 
      </citerefentry>
      and 
      <citerefentry>
        <refentrytitle>mpirun_chgm</refentrytitle>
        <manvolnum>8</manvolnum> 
      </citerefentry>
      .
    </para>
    <para>
      It is also possible to run serial applications, thus
      applications not parallelized with MPI, within &ps;. To execute
      a program, run <filename>psistart</filename>. For more details
      refer to   
      <citerefentry>
        <refentrytitle>psistart</refentrytitle>
        <manvolnum>8</manvolnum> 
      </citerefentry>
      and &psug;.
    </para>
  </section>

  <!-- ============== TCP Bypass ============== -->
  <section id="techdetails_tcp_bypass">
    <title>&ps4; TCP bypass</title>
    <para>
      Beginning with version 4.1, &ps4; offers a feature called "TCP
      bypass", enabling applications based on TCP to use the efficient
      <emphasis>p4sock</emphasis> protcol.  The data will be
      redirected within the kernel to the
      <emphasis>p4sock</emphasis>
      protocol. No modifications to the application are necessary!
    </para>
    <para>
      To automatically enable the TCP bypass, insert a line like
    </para>
    <programlisting>
  env PS_TCP &lt;FirstAddress&gt;-&lt;LastAddress&gt;
    </programlisting>
    <para>
      into the configuration file
      <filename>parastation.conf</filename>, were
      <parameter>FirstAddress</parameter> and
      <parameter>LastAddress</parameter> are the first and last IP
      addresses for which the bypass should be enabled.
    </para>
    <note>
      <para>
      Daemons opening TCP sockets and listening for connections, eg.
      <command>inetd</command>, must be restartet after loading the
      TCP bypass module.
      </para>
    </note>
    <para>
      For more information, see <xref linkend="p4tcp"/>.
    </para>
  </section>

  <!-- ============== Controlling communication paths ============== -->
  <section id="techdetails_compath">
    <title>Controlling &ps4; communication paths</title>
    <para>
      &ps; is able to use different communication paths in parallel,
      see <xref linkend="techdetails_psportlib"/>. In order to
      restrict the paths to use, a number of environment variables are
      recognized by &ps;.
    </para>
    <variablelist>
      <varlistentry>
        <term>
          <envar>PSP_SHAREDMEM</envar>
        </term>
        <listitem>
          <para>
            Don't use shared memory for communication within the same
            node.
          </para>
        </listitem>
      </varlistentry>
<!-- fg: why not implemented?
      <varlistentry>
        <term>
          <envar>PSP_TCP</envar>
        </term>
        <listitem>
          <para>
            Don't use TCP for communication.
          </para>
        </listitem>
      </varlistentry>
-->
      <varlistentry>
        <term>
          <envar>PSP_P4SOCK</envar>
        </term>
        <listitem>
          <para>
            Don't use &ps; p4sock protocol for communication.
          </para>
        </listitem>
      </varlistentry>
<!-- TODO: fg: GM
      <varlistentry>
        <term>
          <envar>PSP_GM</envar>
        </term>
        <listitem>
          <para>
            Don't use GM (Myrinet) for communication.
          </para>
        </listitem>
      </varlistentry>
-->
    </variablelist>
    <para>
      To disable the particular transport, the corresponding variable
      must be set to 0, to enable a transport, the variable must be
      set to 1.
    </para>
    <para>
      It is not possible to dynamically disable TCP as a
      communication path. TCP, if configured, is always used as a
      "last resort" for communication.
    </para>
  </section>
  <!-- ============== Authentification ============== -->
  <section id="techdetails_auth">
    <title>Authentification within &ps4;</title>
    <para>
      Whenever a process of a parallel task is spawned within the
      cluster, &ps; does not authenticate the user. Only the user and
      group id is copied to the remote node and used for starting up
      processes. 
    </para>
    <para>
      Thus, it is not necessary for the user to be known by the
      compute node, e.g. having an entry in
      <filename>/etc/passwd</filename>. On the contrary, the
      administrator may disallow logins for users by removing the
      entries from <filename>/etc/passwd</filename>. Usage of common
      authentification schemes like <filename>NIS</filename>
      is not required and therefore limits user management to the
      frontend nodes.
    </para>
    <para>
      Authentification of users is restricted to login or frontend
      nodes and is outside of the scope of &ps;.
    </para>
  </section>
  <!-- ============== Homogeneous user ID space ============== -->
  <section id="techdetails_userspace">
    <title>Homogeneous user ID space </title>
    <para>
      As explained in the previous section, &ps; uses only user and
      group IDs for starting up remote processes. Therefore, all
      processes will have identical user and group IDs on all nodes.
    </para>
    <para>
      A homogeneous user ID space is streched across the entire
      cluster. 
    </para>
  </section>
  <!-- ============== Single system view ============== -->
  <section id="techdetails_singlesystemview">
    <title>Single system view</title>
    <para>
      The &ps; management tools present information of all nodes, or a
      subset of nodes, if requested, at one place. Also
      commands can be launched on each node and the actions initiated
      by the user or administrator are automatically transfered to the
      destination node(s). Thus, the administrator will have a
      <emphasis>single system view</emphasis> of the cluster.
      <!-- TODO: fg: describe single system view aspects -->
    </para>
  </section>
  <!-- ============== parallel shell tool ============== -->
  <section id="techdetails_parashell">
    <title>Parallel shell tool</title>
    <para>
      Beginning with &ps; version 4.1, a parallel shell tool called
      <filename>psh</filename> will be part of &ps;. This tool allows
      to run commands on all or selected nodes of the cluster in
      parallel. It also presents the output of the commands in a
      sophisticated manner, showing common parts and differences.
      <!-- TODO: fg: describe -->
    </para>
    <para>
      This command is intended not to run interactive commands in
      parallel, but to run a single task in parallel on all or a bunch
      of nodes and prepare the output to be easily read by the user.
    </para>
    <para>
      For more details see 
      <citerefentry>
        <refentrytitle>psh</refentrytitle>
        <manvolnum>8</manvolnum> 
      </citerefentry>.
    </para>
  </section>
  <!-- ============== nodes and cpus ============== -->
  <section id="techdetails_nodescpus">
    <title>Nodes and CPUs</title>
      <!-- TODO: fg: describe more precisely -->
      <para>
        Also &ps; by default tries to use a dedicated CPU per compute
        process, there is currently no way to bind a process to a
        particular CPU within Linux. Therefore, there is no guarantee,
        that each process will use its own CPU. But due to the nature
        of parallel tasks, the operating system scheduler will
        typically distribute each process to its own CPU.
      </para>
  </section>
  <!-- ============== AFS ============== -->
  <section id="techdetails_AFS">
    <title>Integration with AFS</title>
    <para>
      To run parallel tasks spawned by &ps; on clusters using
      <filename>AFS</filename>, &ps; provides the scripts
      <filename>env2tok</filename> and <filename>tok2env</filename>.
    </para>
    <para>
      On the frontend side, calling
    </para>
    <programlisting> . gettoken</programlisting>
    <para>
      will create an environment variable AFS_TOKEN containing an
      encoded token. This variable must be added to the list of
      exported variables
    </para>
    <programlisting>  PSI_EXPORTS="AFS_TOKEN,$PSI_EXPORTS"</programlisting>
    <para>
      In addition, the variable
    </para>
    <programlisting>  PSI_RARG_PRE_0=/some/path/env2tok</programlisting>
    <para>
      must be set. This will call the script
      <filename>env2tok</filename> before running the actual program
      on each node. <filename>env2tok</filename> itself will decode
      the token and will setup the AFS environment.
    </para>
    <note>
      <para>
        The commands <filename>SetToken</filename> and
        <filename>GetToken</filename> must be available on each node,
        which are part of the AFS package. Also, the commands
        <filename>uuencode</filename> and
        <filename>uudecode</filename> must be installed.
      </para>
    </note>
    <para>Script <filename>env2tok</filename>:</para>
    <programlisting>
  #!/bin/bash
  tmp=$IFS
  IFS=" "
  export AFS_TOKEN=`GetToken | uuencode /dev/stdout`
  IFS=$tmp
    </programlisting>
    <para>Script <filename>tok2env</filename>:</para>
    <programlisting>
  #!/bin/bash
  IFS=" "
  echo $AFS_TOKEN | uudecode | SetToken

  exec $*
    </programlisting>
  </section>

  <!-- ============== batch systems ============== -->
  <section id="techdetails_batchsystems">
    <title>Integrating external queuing systems</title>
    <para>
      &ps; can be easily integrated with batch queuing  and scheduling
      systems.  In this case, the queuing system will decide, where
      (and when) to run a parallel task, &ps; will then start, monitor
      and terminate the task. In case of higher priorized jobs, the
      batch system may also suspend a task using the &ps; signal
      forwarding.
    </para>
    <para>
      Integration is done by setting up &ps; environment variables,
      like <envar>PSI_HOSTFILE</envar>. 
      &ps; itself need not be modified in any way.
      It is not necessary to use a
      remote shell (<filename>rsh</filename>) to start
      <filename>mpirun</filename> on the first node of the selected
      partition. The batch system should only run the command on the
      same node where the batch system is running, &ps; will start all
      necessary processes on the remote nodes. For details about
      spawning processes refer to &psug;.
    </para>
    <note>
      <para>
        If an external queuing system is used, the environment
        variable <envar>PSI_NODES_SORT</envar> should be set to
        "none", thus no sorting of any predefined node list will be
        done by &ps;.
      </para>
    </note>
    <para>
      Beginning with version 4.1, &ps; includes its own queuing
      facility. For more details, refer to <xref
      linkend="techdetails_queuing"/> and &psug;.
    </para>

    <section id="config_integrationPBSPro">
      <title>Integration with PBS&nbsp;Pro</title>
      <para>
        Parallel jobs started by PBS&nbsp;Pro using the &ps;
        <filename>mpirun</filename> command
        will be automatically recognized. Due to the environment
        variable <envar>PBS_NODEFILE</envar>, defined by PBS&nbsp;Pro, &ps;
        will automatically setup the <envar>PSI_HOSTFILE</envar> to
        <envar>PBS_NODEFILE</envar> and will also setup
        <envar>PSI_NODES_SORT</envar> no "none".
      </para>
      <para>
        Therefore, &ps; will use the (unsorted) hostfile supplied by
        PBS&nbsp;Pro to startup the parallel task.
      </para>
    </section>
    <section id="config_integrationOpenPBS">
      <title>Integration with OpenPBS</title>
      <para>
        Refer to previous <xref linkend="config_integrationPBSPro"/>.
      </para>
    </section>
    <section id="config_integrationLSF">
      <title>Integration with LSF</title>
      <para>
        As described in the section <xref
        linkend="config_integrationPBSPro"/>, &ps; will also recognize
        the variable <envar>LSB_HOSTS</envar>, provided by LSF.
        This variable holds a list of nodes for the parallel task. It
        is copied to the &ps; variable <envar>PSI_HOSTS</envar>,
        consequently it will be used for starting up the task. In
        addition, the variable <envar>PSI_NODES_SORT</envar> will be
        set to "none", thus no sorting to this list will occure.
      </para>
    </section>
  </section>

  <!-- ============== spare nodes ============== -->
  <section id="techdetails_spareNodes">
    <title>Integrating spare nodes</title>
      <!-- TODO: fg: describe more precisely -->
      <para>
        &ps; is able and will in fact automatically use stand by or
        spare nodes, provided that this nodes are installed and
        connected to the cluster. 
        This kind of nodes can be added to the cluster
        like any regular compute node. &ps; will use this nodes like
        any other node. Consequently, this node type will be not only
        used in "hot standby", in fact it is a real part of the
        cluster. 
      </para>
      <para>
        In case one of the nodes fails, this node will be ignored for
        future tasks without administrator intervention due to the
        load balancing and node monitoring facilites of &ps;. 
      </para>
      <para>
        As a consequence, spare nodes will not only increase the
        overall availablity, but can also be used to increase the
        compute power of the entire cluster.
        <emphasis>Run with (100+x)% compute
        power and in case of a failure, still 100% are left.</emphasis>
      </para>
  </section>
</chapter>

  <!-- Keep this comment at the end of the file
  Local variables:
  mode: xml
  sgml-omittag:nil
  sgml-shorttag:nil
  sgml-namecase-general:nil
  sgml-general-insert-case:lower
  sgml-minimize-attributes:nil
  sgml-always-quote-attributes:t
  sgml-indent-step:2
  sgml-indent-data:t
  sgml-parent-document:("userguide.xml" "book" "book" ("title" "bookinfo"))
  sgml-exposed-tags:nil
  sgml-local-catalogs:nil
  sgml-local-ecat-files:nil
  End:
  -->
