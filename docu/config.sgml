<chapter id="configuration">
  <title>Configuration</title>

  <para>
    After installing the &ps; software successfully, only few modifications to
    the configuration file <xref linkend="parastation_conf"/> have to be made
    in order to enable &ps; on the local cluster.
  </para>

  <section id="config_ps">
    <title>Configuration of the &ps; system</title>
    <para>
      Within this section the basic configuration procedure to enable
      &ps; will be described. 
      It covers the configuration of <!--&ps3; using Myrinet and -->
      &ps4; using TCP/IP (Ethernet) and the optimized &ps4; protocol
      <emphasis>p4sock</emphasis>.
    </para>
    <para>
      The primarily configuration work is reduced to 
      editing the central configuration file <filename>parastation.conf</filename>,
      which is basically located in <filename>/etc</filename>.
    </para>
    <para>
      Usually <filename>/etc/parastation.conf</filename> will be a
      symbolic link pointing to the configuration file
      <filename>parastation.conf</filename> residing in the directory
      <filename>/opt/parastation/config</filename>. This directory is
      usually mounted via NFS on each node. This allows the
      centralized storage of the configuration file and reduces
      the effort to modify this file consistently on all nodes.
    </para>
    <para>
      This section describes all parameters of
      <filename>/etc/parastation.conf</filename> necessary to customize &ps;
      for a dedicated cluster environment. A detailed description of all
      possible configuration parameters in the configuration file can be found
      within the <xref linkend="parastation_conf"/> manual page.
    </para>
    <para>
      It is considered that &ps; is installed in a centralized way to the
      default location <filename>/opt/parastation</filename> and that this
      directory is mounted to this location on all the nodes. In case of a
      distributed installation it has to be guaranteed explicitely that all
      &ps; daemons <xref linkend="psid"/> see configuration files with
      consistent content. This may be ensured by editing the
      <filename>parastation.conf</filename> file on one node and copy the
      adapted file onto all other nodes.
    </para>
    <para>
      The distributed configuration files may explicitely replace the symbolic
      link <filename>/etc/parastation.conf</filename>, which of course is also
      possible for the centralized installation, although not recommended.
      Another possibility for the distributed installation is to keep the
      symbolic link and to modify the file the link is pointing to. Keep in
      mind that this has to be done on <emphasis role="bold">all</emphasis>
      nodes.
    </para>
    <para>
      If &ps; is installed in the recommended way (i.e. centralized
      installation to <filename>/opt/parastation</filename>) only the following
      steps have to be executed on the NFS server:
    </para>
    <orderedlist>
      <listitem>
        <para><emphasis role="bold">Copy template</emphasis></para>
        <para>
          Change to the configuration directory
          <filename>/opt/parastation/config</filename> and copy the
          template configuration file
          <filename>parastation.conf.tmpl</filename> to
          <filename>parastation.conf</filename>.
        </para>
        <para>
          The template file contains all possible parameters known by
          the &ps; daemon <xref linkend="psid"/>. Most of these
          parameters are set to their default value within lines
          marked as comments. Only those that have to be modified in
          order to adapt &ps; to the local environment are enabled.
          Additionally all parameters are exemplified using comments.
          A more detailed description of all the parameters can be
          found in the <xref linkend="parastation_conf"/> manual page.
        </para>
        <para>
          The template file is a good starting point to create a
          working configuration of &ps; for your cluster.
        </para>
        <para>
          Beside basic information about the cluster, this template file
          defines all hardware components &ps; is able to handle. Since this
          definitions require a deeper knowledge of &ps;, it is easier to 
          copy the template file anyway.
        </para>
      </listitem>
      <listitem>
        <para><emphasis role="bold">Adapt the <envar>PS_MODULEPATH</envar>
          environment</emphasis>
        </para>
        <para>
<!--          MOD:  -->
          For &ps; version 4.1 and higher, the modules are loaded
          using the standard Linux modules environment, see
          <citerefentry> <refentrytitle>modprobe</refentrytitle>
          <manvolnum>8</manvolnum> </citerefentry>. The
          <envar>PS_MODULEPATH</envar> variable is therefore silently
          ignored with version 4.1 and higher.
        </para>
        <para>
          For &ps; version up to 4.0.6, it is usually
          not necessary to define this parameter, since &ps;
          tries to find the correct module path by itself. This decision is
          based on information about the kernel currently running on the
          machine.
        </para>
        <para>
          In the rare case that &ps;'s build-in heuristic fails, the module
          path hat to be set explicitly. The administrator has to determine,
          which module version provided within
          <filename>/opt/parastation/bin/modules</filename> is suitable. The
          <envar>PS_MODULEPATH</envar> environment variable has to be set to
          the directory using relative or absolute path names.  In the second
          case, the path name has to be relative to the
          <filename>/opt/parastation</filename> directory.
        </para>
      </listitem>

      <listitem>
        <para><emphasis role="bold"><command>Number of
          nodes</command></emphasis>
        </para>
        <para>
          The parameter <command>NrOfNodes</command> has to be set to the
          actual number of nodes within the cluster. Front end nodes have to
          be considered as part of the cluster. E.g. if the cluster contains 8
          nodes with a fast interconnect plus a front end node then
          <command>NrOfNodes</command> has to be set to 9 in order to allow
          the start of parallel tasks from this machine. 
        </para>
      </listitem>

      <listitem>
        <para><emphasis role="bold"><command>HWType</command></emphasis></para>
        <para>
          In order to tell &ps; which general kind of communication hardware should be
          used, the <command>HWType</command> parameter has to be set. This
          could be changed on a per node basis within the nodes section
          (see below).
        </para>
        <para>
          For clusters running &ps4; utilizing the optimized &ps; communication
          stack on Ethernet hardware of any flavor this parameter has to be
          set to:
        </para>
        <programlisting>  HWType p4sock</programlisting>
        <para>
          If the cluster uses Ethernet without optimized
          communication, i.e. running TCP/IP, you will want to set this
          parameter to:
        </para>
        <programlisting>  HWType ethernet</programlisting>
        <para>
          The values that might be assigned to the <command>HWType</command>
          parameter have to be defined within the
          <filename>parastation.conf</filename> configuration file. Have a
          brief look at the various <command>Hardware</command> sections of
          this file in order to find out which hardware types are actually
          defined.
        </para>
        <para>
          Other possible types are: 
        </para>
        <programlisting>  HWType ib</programlisting>
        <para>
<!--          MOD:  -->
          for Infiniband and
        </para>
        <programlisting>  HWType gm</programlisting>
        <para>
<!--          MOD:  -->
          for Myrinet using GM.
        </para>
        <note>
          <para>
          To enable shared memory communication used within SMP nodes, no
          dedicated hardware entry has to be made. Shared memory support 
          is always enabled by default. As there are no options for 
          shared memory, no dedicated hardware section for this kind of 
          interconnect is provided.
          </para>
        </note>
      </listitem>

      <listitem>
        <para><emphasis role="bold"><command>Nodes</command></emphasis></para>
        <para>
          Furthermore &ps; has to be told which nodes should be part of the
          cluster. The usual way of using the <command>Nodes</command>
          parameter is the environment mode, that is already enabled in the
          template file.
        </para>
        <para>
          The general syntax of the <command>Nodes</command> environment is one
          entry per line. Each entry has the form
        </para>
        <programlisting>  <replaceable>hostname</replaceable> <replaceable>id</replaceable> <optional><option>HWType</option></optional> <optional><option>runJob</option></optional> <optional><option>starter</option></optional> </programlisting>
        <para>
          This will register the node
          <parameter><replaceable>hostname</replaceable></parameter> to the
          &ps; system with the &ps; ID
          <parameter><replaceable>id</replaceable></parameter>. The &ps; ID has
          to be an integer number between 0 and <command>NrOfNodes</command>-1.
        </para>
        <para>
          For each cluster node defined within the <command>Nodes</command>
          environment at least the hostname of the node and the &ps; ID of this
          node have to be given. The optional parameters
          <command>HWType</command>, <command>runJobs</command> and
          <command>starter</command> may be
          ignored for now. For a detailed description of these parameters refer
          to the <xref linkend="parastation_conf"/> manual page.
        </para>
        <para>
          Usually the nodes will be enlisted ordered by increasing &ps; IDs,
          beginning with 0 for the first node. If a front end node exists
          and furthermore should be integrated into the &ps; system, it usually
          should be configured with ID 0.
        </para>
        <para>
          Within an Ethernet cluster the mapping
          between hostnames and &ps; ID is completely unrestricted. 
          <!-- fg: TODO: what about Myrinet, IB ??? -->
        </para>
      </listitem>

      <listitem>
        <para><emphasis role="bold"><command>LicenseServer</command></emphasis>
        </para>
        <para>
<!--          MOD:  -->
          For version 4.1 and up, the license server is obsolete and
          this entry is silently ignored.
        </para>
        <para>
          For versions up to 4.0.6, this parameter defines on which
          node the &ps; license daemon <xref linkend="psld"/> should
          be run. This node must be a member of the cluster, thus &ps;
          has to be installed on this node, too.
        </para>
        <para>
          For clusters with a front end node usually this front end will run
          the license daemon. Otherwise the node with &ps; ID 0 will normally
          adopt this tasks.
        </para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">License key</emphasis></para>
        <para>
          A valid license key is required to run &ps;. In order to obtain a
          license key please contact <email>license@cluster-competence-center.com</email>.
        </para>
        <para>
          Usually the license file will be sent by email. This file should
          be saved to
          <filename>/opt/parastation/config/license</filename>. This is the
          default location where the &ps; daemon <xref linkend="psid"/> is
          expecting the license key. Please consult the manual page of the &ps;
          configuration file <xref linkend="parastation_conf"/> on how to
          modify this default location.
        </para>
      </listitem>
    </orderedlist>
    <para>
<!--      MOD:  -->
      In order to verify the configuration, the command
    </para>
    <programlisting>  /opt/parastation/bin/test_config</programlisting>
    <para>
      could be run. This command will analyze the configuration file
      and report any configuration failures.  After finishing these
      steps, the configuration of &ps; is done. 
    </para>
  </section>

<!-- fg: Beschreibung Myrinet setup komplett geloescht -->

  <section id="Enable_optimized_network_drivers">
<!--    MOD:  -->
    <title>Enable optimized network drivers</title>
    <para>
      As explained in the previous chapter, &ps4; comes with its own
      versions of adapted network drivers for the Intel (e1000) and
      Broadcom (bcm5700) NICs. 
      If the optimized &ps; protocol <command>p4sock</command> is used
      to transfer application data across Ethernet, this adapted
      drivers should be used.
      To enable this drivers, the easiest way
      is to rename the original modules and recreate the module dependencies:
    </para>
    <programlisting>  cd /lib/modules/$(uname -r)/kernel/drivers/net
  mv e1000/e1000.o e1000/e1000-orig.o
  mv bcm/bcm5700.o bcm/bcm5700-orig.o
  depmod -a</programlisting>
    <para>
      If your system uses the e1000 driver, a subsequent
      <command>modinfo</command>
      command should report that the new &ps; version of the driver
      will be used:
    </para>
    <programlisting>  # modinfo e1000
  filename: /lib/modules/2.4.21-198-smp/kernel/drivers/net/ps4/e1000.o
  description: "Intel(R) PRO/1000 Network Driver"
  author:      "Intel Corporation, &lt;linux.nics@intel.com&gt;"
  ...</programlisting>
    <para>
      The "filename" entry reports that the &ps; version of the driver
      will be used.  The same should apply for the
      <command>bcm5700</command> network driver.
    </para>
    <para>
      To reload the new version of the network drivers, it is
      necessary to reboot the system.
    </para>
  </section>

  <section id="testing">
    <title>Testing the installation</title>
<!-- fg: TODO: load modules using rcparastation? -->
    <para>
      After installing and configuring &ps; on each node of the cluster,
      the &ps; daemons can be started up. These daemons will setup all
      necessary communication relations and thus will form the virtual
      cluster consisting of the available nodes.
    </para>
    <para>
      The daemons are started using the
      <command>psiadmin</command> command. This command will establish
      a connection to the local <command>psid</command>. If this daemon 
      is not already up
      and running, the <command>inetd</command> will start up the daemon 
      automatically:
    </para>
    <programlisting>  /opt/parastation/bin/psiadmin </programlisting>
      <para>
        After connecting to the local psid daemon, this command will issue
        a prompt
      </para>
    <programlisting>  <prompt>psiadmin></prompt></programlisting>
      <para>
        To start up the &ps; daemons on all other nodes, use the add
        command:
      </para>
    <programlisting>  <prompt>psiadmin> </prompt><userinput>add</userinput></programlisting>
      <para>
        The following status enquiry command
      </para>
    <programlisting>  <prompt>psiadmin> </prompt><userinput>status</userinput></programlisting>
      <para>
        should list all nodes as "up". To verify that all nodes have
        installed the proper kernel modules, type
      </para>
    <programlisting>  <prompt>psiadmin> </prompt><userinput>status hw</userinput></programlisting>
    <para>
<!--      MOD:  -->
      The command should report for all nodes all hardware types
      configured, eg. <parameter>p4sock</parameter>,
      <parameter>ethernet</parameter>.
    </para>


    <para>
      Alternatively, it is possible to use the single command form of the psiadmin command: 
    </para>
    <programlisting>  /opt/parastation/bin/psiadmin -s -c "status"</programlisting>
    <para>
      The command should be repeated until all nodes are up. 
      The &ps; administration tool is
      described in detail in the corresponding manual page <xref
      linkend="psiadmin"/>.
    </para>
    <para>
      If after a couple of seconds some nodes are still marked as "down", the file
      <filename>/var/log/messages</filename> at this node should be inspected.
      Entries like <quote>psid: ....</quote> at the end of the file
      may report problems or errors.
    </para>
    <para>
      When all nodes are up the communication can be tested using
    </para>
    <programlisting>  /opt/parastation/bin/test_nodes -np <replaceable>nodes</replaceable></programlisting>
    <para>
      where <replaceable>nodes</replaceable> has to be replaced by the actual
      number of nodes within the cluster. After a while a result like
    </para>
    <programlisting>  ---------------------------------------
  Master node 0
  Process 0-31 to 0-31 ( node 0-31 to 0-31 ) OK
  All connections ok
  
  PSIlogger: done </programlisting>
    <para>
      should appear. Of course the number '31' will be replace by a the actual 
      number of nodes given on the command line, i.e. <replaceable>nodes</replaceable>-1.
    </para>
    <para>
      in case of failure, <command>test_nodes</command> may give
      continuously results like
    </para>
    <programlisting>  ---------------------------------------
  Master node 0
  Process 0-2,4-6 to 0-7 ( node 0-2,4-6 to 0-7 ) OK
  Process 3 to 0-6 ( node 3 to 0-6 ) OK
  Process 7 to 0-2,4-7 ( node 7 to 0-2,4-7 ) OK </programlisting>
    <para>
      A detailed description of <command>test_nodes</command> can be found
      within the corresponding manual page <xref linkend="test_nodes"/>.
    </para>
  </section>

<!-- fg: section embed moved to techdetails -->

<!-- fg: no more (native) myrinet -->

</chapter>
  <!-- Keep this comment at the end of the file
  Local variables:
  mode: xml
  sgml-omittag:nil
  sgml-shorttag:nil
  sgml-namecase-general:nil
  sgml-general-insert-case:lower
  sgml-minimize-attributes:nil
  sgml-always-quote-attributes:t
  sgml-indent-step:2
  sgml-indent-data:t
  sgml-parent-document:("adminguide.xml" "book" "book" ("title" "bookinfo"))
  sgml-exposed-tags:nil
  sgml-local-catalogs:nil
  sgml-local-ecat-files:nil
  End:
  -->
