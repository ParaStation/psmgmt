<glossary>

  <glossentry id="ARPfull">
    <glossterm>Address Resolution Protocol</glossterm>
    <glossdef>
      <para>
	A sending host decides, through a protocols routing mechanism, that it
	wants to transmit to a target host located some place on a connected
	piece of a physical network. To actually transmit the hardware packet
	usually a hardware address must be generated. In the case of Ethernet
	this is 48 bit Ethernet address. The addresses of hosts within a
	protocol are not always compatible with the corresponding hardware
	address (being different lengths or values).
      </para>
      <para>
	The Address Resolution Protocol (ARP) is used by the sending host in
	order to resolve the Ethernet address of the target host from its IP
	address. It is described in the <ulink
	  url="http://www.faqs.org/rfcs/rfc826.html">RFC 826</ulink>. The ARP
	is part of the TCP/IP protocol family.
      </para>
    </glossdef>
  </glossentry>

  <glossentry id="AdministrationNetwork">
    <glossterm>Administration Network</glossterm>
    <glossdef>
      <para>
        The administration network is used for exchanging (meta) data
        used for administrative tasks between cluster nodes.
      </para>
      <para>
        This network typically carries only a moderate data rate and
        can be entirely separated from the data network. Almost
        always, Ethernet (Fast or more and more Gigabit) is used for
        this purpose. 
      </para>
    </glossdef>
  </glossentry>

  <glossentry id="ARP">
    <glossterm>ARP</glossterm>
    <glosssee otherterm="ARPfull"/>
  </glossentry>

  <glossentry id="DataNetwork">
    <glossterm>Data Network</glossterm>
    <glossdef>
      <para>
        The data network is used for exchanging data
        between the compute processes on the cluster nodes. Typically,
        high bandwidth and low latency is required for this kind of
        network. 
      </para>
      <para>
        Interconnect types used for this network are Myrinet or
        Infiniband, and (Gigabit) Ethernet for moderate bandwidth and
        latency requirements. 
      </para>
      <para>
        Especially for Ethernet based clusters, the administration and
        data network are often collapsed into a single interconnect.
      </para>
    </glossdef>
  </glossentry>

  <glossentry id="DMA">
    <glossterm>DMA</glossterm>
    <glosssee otherterm="DMAfull"/>
  </glossentry>

  <glossentry id="DMAfull">
    <glossterm>Direct Memory Access</glossterm>
    <glossdef>
      <para>
	In the old days devices within a computer where not able to put data
	into memory on their own but the CPU had to fetch it from them and to
	store it to the final destination manually.
      </para>
      <para>
	Nowadays devices as Ethernet cards, harddisk controllers, Myrinet cards
	etc. are capable to store chunks of data into memory on their
	own. E.g. a disk controller is told to fetch an amount of memory from a
	hard disk and to store it to a given address. The rest of the jobs is
	done by this controller without producing further load to the CPU.
      </para>
      <para>
	Obviously this concept helps to disburden the CPU from work which is
	not its first task and thus gives more power to solve the actual
	application.
      </para>
    </glossdef>
  </glossentry>

  <glossentry id="forwarder">
    <glossterm>Forwarder</glossterm>
    <glosssee otherterm="forwarderfull"/>
  </glossentry>

  <glossentry id="logger">
    <glossterm>Logger</glossterm>
    <glosssee otherterm="loggerfull"/>
  </glossentry>

<!-- fg: nomore 
  <glossentry id="MCP">
    <glossterm>MCP</glossterm>
    <glosssee otherterm="MCPfull"/>
  </glossentry>

  <glossentry id="MCPfull">
    <glossterm>Myrinet Control Program</glossterm>
    <glossdef>
      <para>
	The program controlling the Myrinet card. It is executed by the LanAI
	processor residing on the Myrinet card and thus creates almost no load
	on the node's main processor.
      </para>
    </glossdef>
  </glossentry>
-->

  <glossentry id="MasterNode">
    <glossterm>Master Node</glossterm>
    <glossdef>
      <para>
        Beginning with version 4.1, the evaluation of temporary node
        lists while spawning new tasks is done only by one particular
        <citerefentry> 
          <refentrytitle>psid</refentrytitle>
          <manvolnum>8</manvolnum> 
        </citerefentry>
        within the cluster. The node running this daemon is called
        <emphasis>master node</emphasis>. 
      </para>
      <para>
        The master node is dynamically selected within the cluster and
        may change, if the current master node is no longer available.
        Election is based on the node IDs, refer to
        <citerefentry> 
          <refentrytitle>parastation.conf</refentrytitle>
          <manvolnum>5</manvolnum> 
        </citerefentry>.
      </para>
    </glossdef>
  </glossentry>

  <glossentry id="NICfull">
    <glossterm>Network Interface Card</glossterm>
    <glossdef>
      <para>
	The physical device which connects a computer to a network. Examples
	are Ethernet cards (which are nowadays often found to be on board) or
	Myrinet cards.
      </para>
    </glossdef>
  </glossentry>

  <glossentry id="NIC">
    <glossterm>NIC</glossterm>
    <glosssee otherterm="NICfull"/>
  </glossentry>

  <glossentry id="task">
    <glossterm>Parallel task</glossterm>
    <glossdef>
      <para>
	A bunch of <link linkend="process">processes</link> distributed within
	the cluster forming a parallel application. E.g. a MPI program running
	on several nodes of a cluster can only act as a whole but consists of
	individual processes on each node. &ps; knows about their
	relationship and can handle them as a distributed parallel task
	running on the cluster.
      </para>
      <para>
        Sometimes also referred as <emphasis>job</emphasis>.
      </para>
    </glossdef>
  </glossentry>

  <glossentry id="loggerfull">
    <glossterm>&ps; logger</glossterm>
    <glossdef>
      <para>
        The counterpart to the <xref linkend="forwarderfull"/>. This
        process receives all output collected by the forwarder
        processes and sends it to the final destination, stdout or
        stderr. Furthermore input to the &ps; task is forwarded to a
        specific process.
      </para>
      <para>
        The first process of the task started usually converts to the
        logger processes after spawning all the other processes of the
        parallel task.
      </para>
    </glossdef>
  </glossentry>

  <glossentry id="forwarderfull">
    <glossterm>&ps; forwarder</glossterm>
    <glossdef>
      <para>
	Collects output written by &ps; controlled processes to
	<filename>stdout</filename> or <filename>stderr</filename> and sends it
	to the <xref linkend="loggerfull"/>.
      </para>
      <para>
	Furthermore the forwarder controls the process and sends information
	about its exit status to the local daemon.
      </para>
    </glossdef>
  </glossentry>

  <glossentry id="process">
    <glossterm>Process</glossterm>
    <glossdef>
      <para>
	The atomic part of a <xref linkend="task"/>. A process is at first a
	standard Unix process. Since &ps; knows about its membership in a
	parallel task, it can be handled in a peculiar way if an event takes
	place on some other node (e.g. another process of the task dies
	unexpectedly, a signal is send to the task, etc.).
      </para>
    </glossdef>
  </glossentry>

</glossary>
  <!-- Keep this comment at the end of the file
  Local variables:
  mode: xml
  sgml-omittag:nil
  sgml-shorttag:nil
  sgml-namecase-general:nil
  sgml-general-insert-case:lower
  sgml-minimize-attributes:nil
  sgml-always-quote-attributes:t
  sgml-indent-step:2
  sgml-indent-data:t
  sgml-parent-document:("adminguide.xml" "book" "book" ("title" "bookinfo"))
  sgml-exposed-tags:nil
  sgml-local-catalogs:nil
  sgml-local-ecat-files:nil
  End:
  -->
