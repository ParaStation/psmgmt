<chapter id="installation">
  <title>Installation</title>
  <para>
    Beforehand describing the installation of &ps;, this chapter discusses the
    prerequisites to do so. There are specified three different ways to install
    &ps;, discriminating against each other in the level of automatization of
    the installation process.
  </para>
  <para>
    Of course, the less automatized the choosen way of installation is, the
    more possibilities of costumization within the installation process occur.
    On the other hand even the most automatized way of installation, the
    installation via RPM, should give suitable result in most cases.
  </para>

  <section>
    <title>Prerequisites</title>
    <para>
      To install &ps; on a cluster of workstation, few prerequisites have to be
      met.
    </para>

    <simplesect>
      <title>Hardware</title>
      <para>
	You need a homogenous cluster in the sense of processor architecture,
	i.e. you can mix Intel IA32 and AMD but not Intel IA32 and Alpha. The
	supported processor architecures up to now are Intel IA32 (including
	AMD Athlon) and Alpha.
      </para>
      <para>
	Furthermore of course the nodes need to be interconnected. Usually
	there is a connection via fast or gigabit Ethernet between the nodes.
	&ps; needs this Ethernet interconnect for management purposes. Beyond
	that the possibility to have a fast interconnect in order to do high
	bandwith, low latency communication exists. At the moment Myrinet is
	supported to do this kind of communication. There also exists a version
	supporting communication via fast Ethernet. Naturally this version
	lacks of really high throughput and low latency due to the limited
	capabilities of the hardware.
      </para>
    </simplesect>

    <simplesect>
      <title>Multicast</title>
      <para>
	From the software side of view only very few prerequisites have to be
	met. A Linux kernel supporting multicast on all nodes of the cluster is
	needed. This is usually no problem if a standard kernel from a common
	distribution is used. You have to explicitly enable multicast support
	if you build your own kernel. In order to learn more about multicast
	take a look at the <ulink
	  url="http://jungla.dit.upm.es/~jmseyas/linux/mcast.howto/multic.html"> 
	  <citetitle>Multicast over TCP/IP HOWTO</citetitle></ulink>.
      </para>
      <para>
	Besides the kernel supporting multicast, the hardware has to support
	multicast, too. Since modern Ethernet switches support multicast and
	the nodes of a cluster of workstation usually live in a private subnet,
	there should be no problem about this. In the case of a gateway in
	between the nodes of a cluster this gateway has to be configured
	appropriately to allow multicast packets to reach all nodes of the
	cluster from all nodes.
      </para>
      <para>
	On nodes with more than one Ethernet interface you have to setup a
	defaultroute for the multicast traffic. This is done by
      </para>
      <programlisting>	route add -net 224.0.0.0 netmask 240.0.0.0 dev <replaceable>ethX</replaceable></programlisting>
      <para>
	manually, where <replaceable>ethX</replaceable> has to be replaced by
	the name of the interface connecting to all other nodes. In order to
	enable this route at system startup, a corresponding entry has to be
	added to <filename>/etc/route.conf</filename> or
	<filename>/etc/sysconfig/networks/routes</filename>, depending on the
	type of Linux distribution in use.
      </para>
      <para>
	Usually only the front-end machine of a cluster has two Ethernet
	interfaces in use. Then you have to add the Multicast route to the
	interface connecting the front-end to all other nodes of the cluster.
      </para>
    </simplesect>

    <simplesect>
      <title>Kernel version</title>
      <para>
	Since kernel modules are needed for the high bandwidth, low latency
	communication, a kernel version supported by the modules within the
	&ps; distribution is needed. These modules support the standard kernels
	of common distributions, even if you have recompiled theses kernels in
	a slightly modified way. They are not needed for the communication via
	fast Ethernet.
      </para>
      <para>
	If there is the need for special version of the modules, it is possible
	to get them on demand. Please contact
	<email>support@par-tec.com</email>.
      </para>
    </simplesect>
  </section>

  <section>
    <title>Centralized or distributed installation</title>
    <para>
      The most imported question to answer before starting the installation is
    </para>
    <itemizedlist>
      <listitem>
	<para>
	  whether the &ps; files should reside physically on one node and will
	  be mounted on all the other nodes using NFS (this will called
	  <quote>centralized</quote> installation within this document),
	</para>
      </listitem>
      <listitem>
	<para>
	  or whether they should be stored identically in a distributed fashion
	  on every node (which will be called <quote>distributed</quote>
	  installation).
	</para>
      </listitem>
    </itemizedlist>
    <para>
      Both ans&auml;tze have their pros and cons. On the one hand a centralized
      installation saves from inconsistent versions of executables or
      configuration files within the cluster, but on the other hand it
      introduces a possible single point of failure, the NFS server. This
      disadvantage may be reasonable if a front-end machine acts as a NFS
      server, since all other nodes are unavailable anyhow, if the front-end
      fails. Furthermore the <filename>/home</filename> directory is ususally
      stored centralized, too. Thus doing so with the &ps; files (ideally on
      the same machine) will cause no further harm to the stability and
      availability of the cluster.
    </para>
    <para>
      &ps; supports both methods of installation: It can either be installed on
      every node (residing per default in
      <filename>/opt/parastation</filename>) or it may be installed on a front
      end machine and mounted on everey node of the cluster (ideally also to
      <filename>/opt/parastation</filename>). Be aware of the fact that even if
      &ps; is installed on a front-end machine, some configuration work on each
      node has to be done anyway.
    </para>
    <para>
      Within the discussion of the different installation methods below both
      modi will be described, the <quote>distributed</quote> and the
      <quote>centralized</quote> one.
    </para>
    <para>
      From the authors point of view the benefits of the centralized
      installation outweigh its disadvantages and furthermore the advantages of
      the distributed installation. Thus putting &ps; on a NFS server and
      mounting the directory on all the nodes is the method of choice.
    </para>
    <note>
      <para>
      Experience has shown that mounting &ps; or even
	<filename>/home</filename> via NFS using <citerefentry>
	  <refentrytitle>automount</refentrytitle>
	  <manvolnum>8</manvolnum>
      </citerefentry> may cause major problems during startup of &ps; or
	application startup. This is due to the fact that requests to the NFS
	server might timeout if many clients try to mount directories almost
	concurrently. On the other hand there is no real need using
      <citerefentry>
	  <refentrytitle>automount</refentrytitle>
	  <manvolnum>8</manvolnum>
      </citerefentry> in this case, since the nodes of a cluster usually cannot
	be operated in a usable way without the users'
	<filename>/home</filename> directories. Therefor static NFS mounting
	seems to be reasonable in this case.
      </para>
    </note>
    <section id="dir_struct">
      <title>Directory structure</title>
      <para>
	The default location to install &ps; is
	<filename>/opt/parastation</filename>. Underneath this directory,
	several subdirectories are created containing the actual &ps;
	installation:
      </para>
      <variablelist>
	<varlistentry>
	  <term>
	    <filename>bin</filename>
	  </term>
	  <listitem>
	    <para>
	      contains all executables building the &ps; system. Remember to
	      include this directory into the <envar>PATH</envar> environment.
	    </para>
	  </listitem>
	</varlistentry>
	<varlistentry>
	  <term>
	    <filename>config</filename>
	  </term>
	  <listitem>
	    <para>
	      contains the installation script
	      <filename>PSM_INSTALL</filename>, the example configuration file
	      <filename>parastation.conf.tmpl</filename> and some example
	      routing files for the &ps; Myrinet driver.
	    </para>
	    <para>
	      Furthermore within the standard installation the main
	      configuration file <filename>parastation.conf</filename> is
	      physically located here. Through a symbolic link this
	      configuration file also available in <filename>/etc</filename>,
	      where it is expected by the &ps; daemon <xref
		linkend="psid"/>. This symbolic link will be created by
	      <command>PSM_INSTALL</command> during installation.
	    </para>
	  </listitem>
	</varlistentry>
	<varlistentry>
	  <term>
	    <filename>doc</filename>
	  </term>
	  <listitem>
	    <para>
	      contains the &ps; documentation after installing the
	      corresponding RPM or <filename>.tar.gz</filename> file. The
	      necessary steps are described in <xref linkend="docu_install"/>.
	    </para>
	  </listitem>
	</varlistentry>
	<varlistentry>
	  <term>
	    <filename>include</filename>
	  </term>
	  <listitem>
	    <para>
	      contains the header files needed in order to build &ps;
	      applications. These are primarily needed for building
	      applications using the low level PSPort or PSHAL libraries.
	    </para>
	    <para>
	      These header files are not needed if only self compiled MPI
	      application or precompiled third party applications are used.
	    </para>
	  </listitem>
	</varlistentry>
	<varlistentry>
	  <term>
	    <filename>lib</filename>
	  </term>
	  <listitem>
	    <para>
	      contains various libraries needed in order to build &ps;
	      applications. These are primarily needed for building MPI
	      applications or such using the low level PSPort or PSHAL
	      libraries.
	    </para>
	  </listitem>
	</varlistentry>
	<varlistentry>
	  <term>
	    <filename>man</filename>
	  </term>
	  <listitem>
	    <para>
	      contains the manual pages describing the &ps; daemons, utilities
	      and configuration files after installing the documentation RPM or
	      <filename>.tar.gz</filename> file. The necessary step are
	      described in <xref linkend="docu_install"/>.
	    </para>
	    <para>
	      In order to enable this pages to the user via the
	      <citerefentry>
		<refentrytitle>man</refentrytitle>
		<manvolnum>1</manvolnum>
	    </citerefentry> command please consult the corresponding
	      documentation how to do this<footnote>
		<para>
		  Usually this is either done by modifying the
		  <envar>MANPATH</envar> environment variable or by editing the
		  <citerefentry>
		    <refentrytitle>manpath</refentrytitle>
		    <manvolnum>1</manvolnum>
		  </citerefentry> configuration file, which is
		  <filename>/etc/manpath.config</filename> by default.
		</para>
	      </footnote>.
	    </para>
	  </listitem>
	</varlistentry>
	<varlistentry>
	  <term>
	    <filename>mpich</filename>
	  </term>
	  <listitem>
	    <para>
	      contains a adapted version of <ulink
		url="http://www.unix.mcs.anl.gov/mpi/mpich">MPIch</ulink> after
	      installing the ParaStation/MPIch RPM or
	      <filename>.tar.gz</filename> file. The necessary steps are
	      described in <xref linkend="mpich_install"/>.
	    </para>
	  </listitem>
	</varlistentry>
      </variablelist>
    </section>
  </section>

  <section>
    <title>Installation via rpm</title>
    <para>
      The easiest way to install &ps; is the installation via RPM. This is the
      preferred method on all <ulink url="http://www.suse.de">SuSE</ulink> or
      <ulink url="http://www.redhat.com">RedHat</ulink> based systems.
    </para>
    <simplesect>
      <title>Getting the &ps; RPM file</title>
      <para>
	A RPM containing &ps; can be obtained from the <ulink
	  url="http://www.par-tec.com/downloads/download.html">download
	  section</ulink> of the <ulink url="http://www.par-tec.com">ParTec
	  homepage</ulink>. The file of the linux version is usually named
	<filename>psm-x.y.z-yyyymmdd.Linux.arch.rpm</filename>, where
	<filename>x.y.z</filename> is the version number,
	<filename>yyyymmdd</filename> the release date and
	<filename>arch</filename> is the architecture, i.e. one of
	<filename>i386</filename><!--, <filename>x86-64</filename>,
	<filename>ia64</filename>--> or <filename>alpha</filename>.
      </para>
      <para>
	For the Tru64 version naming is roughly the same only replacing
	<quote>Linux</quote> by <quote>OSF1</quote>.
      </para>
      <para>
	The detailed naming of the RPM file may vary depending on the actual
	version.
      </para>
    </simplesect>
    <simplesect>
      <title>Distributed installation</title>
      <para>
	For the distributed installation, after becoming <quote>root</quote>
	just do a
      </para>
      <programlisting>	rpm -U psm-x.y.z-yyyymmdd.Linux.arch.rpm</programlisting>
      <para>
	<command>rpm</command> will install all the necessary file to
	<filename>/opt/parastation</filename> and call the script
	<command>PSM_INSTALL</command> residing in the
	<filename>config</filename> directory, which will modify some
	configuration files in <filename>/etc</filename>. The modifications
	done by this script are discussed in some detail in <xref
	  linkend="Install_by_hand"/>.
      </para>
      <para>
	This has to be done on all nodes.
      </para>
    </simplesect>
    <simplesect>
      <title>Centralized installation</title>
      <para>
	In order to install &ps; from RPM in a centralized fashion, after
	becoming <quote>root</quote> use the command
      </para>
      <programlisting>	rpm -U psm-x.y.z-yyyymmdd.Linux.arch.rpm</programlisting>
      <para>
	on the NFS server machine to put all the &ps; files onto disk. Make
	sure that <filename>/opt/parastation</filename> is exported and the NFS
	server is running (use <citerefentry>
	  <refentrytitle>showmount</refentrytitle>
	  <manvolnum>8</manvolnum>
	</citerefentry> to do so). Then mount the exported directory on all
	nodes to <filename>/opt/parastation</filename>. Afterwards run
      </para>
      <programlisting>	/opt/parastation/config/PSM_INSTALL -i</programlisting>
      <para>
	manually on all the nodes. The modifications done are discussed in
	<xref linkend="Install_by_hand"/>.
      </para>
    </simplesect>
  </section>

  <section>
    <title>Installation using <filename>.tar.gz</filename> files</title>
    <para>
      If the linux distribution in use does not support RPM, the apropriate way
      to install &ps; is the one using the <filename>.tar.gz</filename> file.
    </para>
    <simplesect>
      <title>Getting the &ps; <filename>.tar.gz</filename> file</title>
      <para>
	A <filename>.tar.gz</filename> file containing &ps; can be obtained
	from the <ulink
	  url="http://www.par-tec.com/downloads/download.html">download
	  section</ulink> of the <ulink url="http://www.par-tec.com">ParTec
	  homepage</ulink>. The file of the linux version is usually named
	<filename>psm-x.y.z-yyyymmdd.Linux.arch.tar.gz</filename>, where
	<filename>x.y.z</filename> is the version number,
	<filename>yyyymmdd</filename> the release date and
	<filename>arch</filename> is the architecture, i.e. one of
	<filename>i386</filename><!--, <filename>x86-64</filename>,
	<filename>ia64</filename>--> or <filename>alpha</filename>.
      </para>
      <para>
	For the Tru64 version naming is pretty much the same only replacing
	<quote>Linux</quote> by <quote>OSF1</quote>.
      </para>
      <para>
	The detailed naming of the <filename>.tar.gz</filename> file might vary
	depending on the actual version.
      </para>
    </simplesect>
    <simplesect>
      <title>Distributed installation</title>
      <para>
	In the case of a distributed installation become <quote>root</quote>
	and change to the root directory <filename>/</filename>. Then just do a
      </para>
      <programlisting>	gzip -cd psm-x.y.z-yyyymmdd.Linux.arch.tar.gz | tar xvf -</programlisting>
      <para>
	This will install all the necessary file to
	<filename>/opt/parastation</filename>. Afterwards the script
	<command>PSM_INSTALL</command> residing in the
	<filename>config</filename> directory has to be called:
      </para>
      <programlisting>	/opt/parastation/config/PSM_INSTALL -i</programlisting>
      <para>
	This will modify some configuration files in <filename>/etc</filename>.
	The modifications done by this script are discussed in detail in <xref
	  linkend="Install_by_hand"/>.
      </para>
      <para>
	These steps have to be executed on all nodes of the cluster.
      </para>
    </simplesect>
    <simplesect>
      <title>Centralized installation</title>
      <para>
	The installation of &ps; from a <filename>.tar.gz</filename> file in a
	centralized fashion is done in the following steps: Become
	<quote>root</quote> and change to the root directory
	<filename>/</filename>. Then use the command
      </para>
      <programlisting>	gzip -cd psm-x.y.z-yyyymmdd.Linux.arch.tar.gz | tar xvf -</programlisting>
      <para>
	on the NFS server machine to put all the &ps; files onto disk, i.e. to
	<filename>/opt/parastation</filename>. Enabling &ps; on this machine is
	done by executing the script <command>PSM_INSTALL</command> residing in
	&ps;'s <filename>config</filename> directory:
      </para>
      <programlisting>	/opt/parastation/config/PSM_INSTALL -i</programlisting>
      <para>
	The modifications done by this script will be discussed in <xref
	  linkend="Install_by_hand"/>.
      </para>
      <para>
	Now make sure that <filename>/opt/parastation</filename> is exported
	and the NFS server is running (use <citerefentry>
	  <refentrytitle>showmount</refentrytitle>
	  <manvolnum>8</manvolnum>
	</citerefentry> to do so). Then mount the exported directory on all
	nodes to <filename>/opt/parastation</filename> and run
      </para>
      <programlisting>	/opt/parastation/config/PSM_INSTALL -i</programlisting>
      <para>
	manually on all nodes.
      </para>
    </simplesect>

  </section>

  <section id="Install_by_hand">
    <title>Installation by hand</title>
    <para>
      For full controll on the installation process you might not want to run
      the <command>PSM_INSTALL</command> script but do the necessary
      modifications by hand. Therefor the changes made by this script will be
      discussed now step by step.
    </para>
    <note>
      <para>
	You have to be root to execute the following steps.
      </para>
    </note>

    <orderedlist>
      <listitem id="ibh_create_dev">
	<para><emphasis role="bold">Create the device entry</emphasis></para>
	<para>
	  If not yet done (e.g. from a previous installation), you must create
	  a character device:
	</para>
	<programlisting>	mknod /dev/psm c 40 0
	chmod 666 /dev/psm</programlisting>
	<note>
	  <para>
	    This has only to be done for the Myrinet version of &ps;. You can
	    leave out this step within the installation procedure of
	    ParaStation FE.
	  </para>
	</note>
      </listitem>
      <listitem id="ibh_services">
	<para><emphasis role="bold">Services</emphasis></para>
	<para>
	  The following lines must be appended to
	  <filename>/etc/services</filename>:
	</para>
	<programlisting>
	# ParaStation entries
	psld          887/tcp     # ParaStation License Daemon Start Port
	psid          888/tcp     # ParaStation Daemon Start Port
	# end of ParaStation entries</programlisting>
	<para>
	  These ports are used by the automatic startup mechanism of the &ps;
	  daemon <xref linkend="psid"/> and license daemon <xref
	    linkend="psld"/> via <citerefentry>
	    <refentrytitle>inetd</refentrytitle>
	    <manvolnum>8</manvolnum>
	  </citerefentry> or <citerefentry>
	    <refentrytitle>xinetd</refentrytitle>
	    <manvolnum>8</manvolnum>
	  </citerefentry>.
	</para>
      </listitem>
      <listitem id="ibh_inetd">
	<para><emphasis role="bold">(X)inetd Configuration</emphasis></para>
	<para>
	  This part of the installation process differs for the various Linux
	  distributions. If your system has
	  <itemizedlist>
	    <listitem>
	      <para>
		a file named <filename>/etc/inetd.conf</filename> (e.g. a SuSE
		distributions), read section <xref
		  linkend="ibh_inetd_suse"/>,
	      </para>
	    </listitem>
	    <listitem>
	      <para>
		a directory <filename>/etc/xinetd.d</filename> (e.g. a RedHat
		distributions), read section <xref linkend="ibh_inetd_rh"/>.
	      </para>
	    </listitem>
	  </itemizedlist>
	</para>
	<orderedlist numeration="arabic" inheritnum="inherit">
	  <listitem id="ibh_inetd_suse">
	    <para><emphasis role="bold">Inetd Configuration</emphasis></para>
	    <para>
	      The following lines must be appended to
	      <filename>/etc/inetd.conf</filename>:
	    </para>
	    <programlisting>
	#
	# ParaStation daemon
	#
	psid stream tcp nowait root /opt/parastation/bin/psid psid
	#
	# ParaStation license server
	#
	psld stream tcp nowait root /opt/parastation/bin/psld psld</programlisting>
	    <para>
	      Now, the <citerefentry>
		<refentrytitle>inetd</refentrytitle>
		<manvolnum>8</manvolnum>
	      </citerefentry>
	      must be triggered to reload this file:
	    </para>
	    <programlisting>	/etc/init.d/inetd reload</programlisting>
	  </listitem>
	  <listitem id="ibh_inetd_rh">
	    <para><emphasis role="bold">Xinetd Configuration</emphasis></para>
	    <para>
	      Create a file named
	      <filename>/etc/xinetd.d/parastation</filename> with the following
	      contents:
	    </para>
	    <programlisting>
	#
	# ParaStation daemon
	#
	service psid
	{
	disable         = no
	socket_type     = stream
	wait            = no
	user            = root
	server          = /opt/parastation/bin/psid
	server_args     =
	log_on_failure  += USERID
	}
	#
	# ParaStation license server
	#
	service psld
	{
	disable         = no
	socket_type     = stream
	wait            = no
	user            = root
	server          = /opt/parastation/bin/psld
	server_args     =
	log_on_failure  += USERID
	}</programlisting>
	    <para>
	      Now, the xinetd daemon must be triggered to read this file:
	    </para>
	    <programlisting>	/etc/init.d/xinetd reload</programlisting>
	  </listitem>
	</orderedlist>
      </listitem>
    </orderedlist>
  </section>

  <section id="docu_install">
    <title>Installation of the documentation</title>
    <para>
      The &ps; documentation is delivered in three format: A PDF file, a
      browsable HTML documentation and manual pages for all &ps; programs and
      configuration files.
    </para>
    <para>
      In order to install the documentation files first of all an up to date
      version of the documentation has to be retrieved from the <ulink
	url="http://www.par-tec.com/downloads/download.html">download
	section</ulink> of the <ulink url="http://www.par-tec.com">ParTec
	homepage</ulink>. The rpm file is usually named
      <filename>psm_docu-x.y.z-noarch.rpm</filename>, where
      <filename>x.y.z</filename> is the &ps; version number. A corresponding
      <filename>.tar.gz</filename> file may be found at the same location but
      named <filename>psm_docu-x.y.z.tar.gz</filename>.
    </para>
    <para>
      For the installation of the rpm file, simply execute
    </para>
    <programlisting>	rpm -U psm_docu-x.y.z-noarch.rpm</programlisting>
    <para>
      Similary for the <filename>.tar.gz</filename> file change to the root
      directory <filename>/</filename> and execute
    </para>
    <programlisting>	gzip -cd psm_docu-x.y.z.tar.gz | tar xvf -</programlisting>
    <para>
      All the PDF and HTML files of the documentation will be installed into
      <filename>/opt/parastation/doc</filename>, the manual pages will reside
      in <filename>/opt/parastation/man</filename>.
    </para>
    <para>
      The intendet starting point for the HTML documentation is
      <filename>file:///opt/parastation/doc/html/index.html</filename>. The PDF
      documentation is included in two PDF file called
      <filename>adminguide.pdf</filename> for the <citetitle>&ps;
	Administrators Guide</citetitle> and <filename>userguide.pdf</filename>
      for the <citetitle>&ps; Users Guide</citetitle>. Both may be found in the
      <filename>/opt/parastation/doc/pdf</filename> directory. The PDF
      documentation can be viewed using the Acrobat Reader, which can be
      retrieved from the <ulink url="http://www.adobe.com">Adobe
	homepage</ulink>.
    </para>
    <para>
      Hardcopies of the &ps; documentation can be obtained from ParTec,
      although it is much easier for the customer to just print out the PDF
      file using the Acrobat Reader.
    </para>
    <para>
      In order to enable the manual pages to the users please consult the
      documentation of the <citerefentry>
	<refentrytitle>man</refentrytitle>
	<manvolnum>1</manvolnum>
      </citerefentry> command and the remark in <xref linkend="dir_struct"/> on
      how to do this.
    </para>
  </section>

  <section id="mpich_install">
    <title>Installation of MPIch</title>
    <para>In order to enable MPI over &ps; might be enabled on the cluster by
      installing an adapted version of <ulink
	url="http://www-unix.mcs.anl.gov/mpi/mpich/">MPIch</ulink>. A
      corresponding packet can be found within the <ulink
	url="http://www.par-tec.com/downloads/download.html">download
	section</ulink> of the <ulink url="http://www.par-tec.com">ParTec
	homepage</ulink>. The file of the linux version is usually named
      <filename>mpich_a.b.cpsx.y.z_arch.tar.gz</filename>, where
      <filename>a.b.c</filename> is the MPIch version number,
      <filename>x.y.z</filename> is the &ps; version number and
      <filename>arch</filename> is the architecture, i.e. one of
      <filename>i386</filename><!--, <filename>x86-64</filename>,
      <filename>ia64</filename>--> or <filename>alpha</filename>.
    </para>
    <para>
      The corresponding file for Tru64 has the name
      <filename>mpich_a.b.cpsx.y.z_tru64.tar.gz</filename>.
    </para>
    <note>
      <para>
	The &ps; version number may differ from the one of the latest &ps;
	release. This is due to the fact that bug fixes within &ps; that don't
	break compatibility of linked programs result in a change of the minor
	version number. Since the old version of MPIch is still compatible, it
	is not necessary to rebuild the MPIch distribution in this case.
      </para>
    </note>
    <para>
      After downloading the correct MPIch distribution file make sure to be
      root and change to the root directory <filename>/</filename>. Then
      unpack the <filename>.tar.gz</filename> file using
    </para>
    <programlisting>
	cd /
	gzip -cd mpich_a.b.cpsx.y.z_arch.tar.gz | tar xvf -</programlisting>
    <para>
      This will extract the ParaStation/MPIch distribution to
      <filename>/opt/parastation/mpich</filename>. Since all these files are
      only needed for building MPI applications, but not for running them, it
      is sufficient to do the extraction only on the nodes used for compilation
      or the front-end machine.
    </para>
    <para>
      In order to enable the MPI commands to the users make sure that
      <filename>/opt/parastation/mpich/bin</filename> is included into the
      system wide <envar>PATH</envar> environment variable. Furthermore you
      might want to enable the MPI man pages. These reside in
      <filename>/opt/parastation/mpich/man</filename>. Please consult the
      documentation of the <citerefentry>
	<refentrytitle>man</refentrytitle>
	<manvolnum>1</manvolnum>
      </citerefentry> command and the remark in <xref linkend="dir_struct"/> on
      how to do this.
    </para>
  </section>

  <section>
    <title>Further steps</title>
    <para>
      With this the basic installation of &ps; is done. There remain some
      further step until &ps; can really be used. These include:
    </para>
    <itemizedlist>
      <listitem>
	<para>
	  <link linkend="config_ps">the configuration of the &ps; system</link>
	</para>
      </listitem>
      <listitem>
	<para>
	  <link linkend="setup_route">setting up the routing for Myrinet
	    communication</link>
	</para>
      </listitem>
      <listitem>
	<para>
	  <link linkend="testing">testing if everything works well</link>
	</para>
      </listitem>
      <listitem>
	<para>
	  <link linkend="embed">optionally the embedding of the &ps; system
	    into an allready existing batch system.</link>
	</para>
      </listitem>
    </itemizedlist>
    <para>
      These step will be discussed in the next chapter of this document.
    </para>
  </section>

  <section>
    <title>Uninstall</title>
    <para>
      In order to remove &ps; from the cluster it was installed on there are
      different ways to do so depending on the choosen way of installation.
    </para>
    <simplesect>
      <title>Centralized installations</title>
      <para>
	First of all for any centralized installation you have to run
      </para>
      <programlisting>	/opt/parastation/config/PSM_INSTALL -u</programlisting>
      <para>
	on all nodes that have mounted <filename>/opt/parastation</filename>
	from the NFS server unless you want to remove the modifications done
	while installation manually. In that case consult <xref
	linkend="Install_by_hand"/> to find out which steps have to be made
	undone.
      </para>
      <para>
      Now it is save to remove all files installed on the the NFS server. If
	the installation was made via <citerefentry>
	  <refentrytitle>rpm</refentrytitle>
	  <manvolnum>1</manvolnum>
      </citerefentry> the corresponding packet simply has to be remove using
      </para>
      <programlisting>	rpm -e psm</programlisting>
      <para>
	For installations done from the <filename>.tar.gz</filename> first undo
	all modification to configuration files within
	<filename>/etc</filename> by running
      </para>
      <programlisting>	/opt/parastation/config/PSM_INSTALL -u</programlisting>
      <para>
	Afterwards /opt/parastation can be removed savely.
      </para>
    </simplesect>
    <simplesect>
      <title>Distributed installation</title>
      <para>
	If the installation was done from the RPM file, simply execute a
      </para>
      <programlisting>	rpm -e psm</programlisting>
      <para>
	on all nodes of the cluster. Otherwise
      </para>
      <programlisting>	/opt/parastation/config/PSM_INSTALL -u</programlisting>
      <para>
	has to be called on every node of the cluster and
	<filename>/opt/parastation</filename> can be removed afterwards.
      </para>
    </simplesect>
  </section>
</chapter>
  <!-- Keep this comment at the end of the file
  Local variables:
  mode: xml
  sgml-omittag:nil
  sgml-shorttag:nil
  sgml-namecase-general:nil
  sgml-general-insert-case:lower
  sgml-minimize-attributes:nil
  sgml-always-quote-attributes:t
  sgml-indent-step:2
  sgml-indent-data:t
  sgml-parent-document:("adminguide.xml" "book" "book" ("title" "bookinfo"))
  sgml-exposed-tags:nil
  sgml-local-catalogs:nil
  sgml-local-ecat-files:nil
  End:
  -->
