<chapter id="installation">
  <title>Installation</title>
  <para>
    This chapter describes the installation of &ps;. Beforehand the
    prerequisites to use &ps; are discussed. There are three different ways to
    install &ps;, discriminating against each other in the level of
    automatization of the installation process.
  </para>
  <para>
    Of course, the less automatized the choosen way of installation is, the
    more possibilities of costumization within the installation process occur.
    On the other hand even the most automatized way of installation, the
    installation via RPM, should give suitable result in most cases.
  </para>

  <section>
    <title>Prerequisites</title>
    <para>
      In order to enable a cluster of workstation for the installation of the
      &ps; communication system, few prerequisites have to be met.
    </para>

    <simplesect>
      <title>Hardware</title>
      <para>
	You need a homogenous cluster in the sense of processor architecture,
	i.e. you can mix Intel IA32 and AMD but not Intel IA32 and Alpha. The
	supported processor architecures up to now are Intel IA32 (including
	AMD Athlon) and Alpha.
      </para>
      <para>
	Furthermore of course the nodes need to be interconnected. In
	principle, &ps; needs two different kinds of interconnect. On the one
	hand there is the need for a so called administration network which is
	used to handle all the administrative tasks that have to be dealed with
	within a cluster. Besides the conventional sharing of NFS partitions or
	NIS tables, on a &ps; cluster this also includes the inter-daemon
	communication used to implement the effective cluster administration
	and job handling mechanisms. This administration network is usually
	implemented using a Fast or Gigabit Ethernet network.
      </para>
      <para>
	On the other hand, there is the equally important need for a high speed
	interconnect in order to do high bandwidth, low latency communication
	within parallel applications. While historically this kind of
	communication is usually done via Myrinet, nowadays Gigabit Ethernet is
	a much cheaper and only slighty slower alternative. Furthermore, with
	the dawn of Infiniband another alternative is in sight. &ps;
	supports Myrinet and Ethernet now and will support Infiniband soon.
      </para>
      <para>
	In the special case of an Ethernet communication interconnect it is not
	necessary to really have to distinct networks but rather possible to
	use one physical network for both tasks.
      </para>
    </simplesect>

    <simplesect>
      <title>Multicast</title>
      <para>
	From the software side of view only very few prerequisites have to be
	met. A Linux kernel supporting multicast on all nodes of the cluster is
	needed. This is usually no problem, if a standard kernel from a common
	distribution is used. You have to explicitly enable multicast support
	if you build your own kernel. In order to learn more about multicast
	take a look at the <ulink
	  url="http://www.linux.org/docs/ldp/howto/Multicast-HOWTO.html"> 
	  <citetitle>Multicast over TCP/IP HOWTO</citetitle></ulink>.
      </para>
      <para>
	Besides the kernel supporting multicast, the hardware has to support
	multicast, too. Since modern Ethernet switches support multicast and
	the nodes of a cluster of workstation usually live in a private subnet,
	there should be no problem about this. In the case of a gateway in
	between the nodes of a cluster this gateway has to be configured
	appropriately to allow multicast packets to reach all nodes of the
	cluster from all nodes.
      </para>
      <para>
	On nodes with more than one Ethernet interface you have to setup a
	default route for the multicast traffic. This is done by
      </para>
      <programlisting>	route add -net 224.0.0.0 netmask 240.0.0.0 dev <replaceable>ethX</replaceable></programlisting>
      <para>
	manually, where <replaceable>ethX</replaceable> has to be replaced by
	the name of the interface connecting to all other nodes. In order to
	enable this route at system startup, a corresponding entry has to be
	added to <filename>/etc/route.conf</filename> or
	<filename>/etc/sysconfig/networks/routes</filename>, depending on the
	type of Linux distribution in use.
      </para>
      <para>
	Usually only the front-end machine of a cluster has two Ethernet
	interfaces in use. Then you have to add the Multicast route to the
	interface connecting this machine to all other nodes of the cluster.
      </para>
    </simplesect>

    <simplesect>
      <title>Kernel version</title>
      <para>
	Since kernel modules are needed for the high bandwidth, low latency
	communication in the &ps3; and &ps4; variants of the software, a kernel
	version supported by the modules within the &ps; distribution is
	needed. The modules that come within this &ps; distribution support the
	standard kernels of common Linux distributions, even if you have
	recompiled these kernels in a slightly modified way.
      </para>
      <para>
	Since the &psfe; variant of the software does not depend on any kernel
	module, no special kernel version is needed in this case.
      </para>
      <para>
	If there is the need for special version of the modules, it is possible
	to get them on demand. In order to request it, please contact
	<email>support@par-tec.com</email>.
      </para>
    </simplesect>
  </section>

  <section>
    <title>Centralized or distributed installation</title>
    <para>
      The most imported question to answer before starting the installation is
    </para>
    <itemizedlist>
      <listitem>
	<para>whether the &ps; files should reside physically on one node and
	  will be mounted on all the other nodes using NFS (and what will be
	  called <quote>centralized</quote> installation within this document),
	</para>
      </listitem>
      <listitem>
	<para>or whether they should be stored identically in a distributed
	  fashion on every node (which will be called
	  <quote>distributed</quote> installation).
	</para>
      </listitem>
    </itemizedlist>
    <para>
      Both approaches have their pros and cons. On the one hand a centralized
      installation saves from inconsistent versions of executables or
      configuration files within the cluster, but on the other hand it
      introduces a possible single point of failure, the NFS server. This
      disadvantage may be reasonable if a front-end machine acts as a NFS
      server, since all other nodes are unavailable anyhow, if the front-end
      fails. Furthermore the <filename>/home</filename> directory is ususally
      stored centralized, too. Thus doing so with the &ps; files (ideally on
      the same machine) will cause no further harm to the stability and
      availability of the cluster.
    </para>
    <para>
      &ps; supports both methods of installation: It can either be installed on
      every node (residing per default in
      <filename>/opt/parastation</filename>) or it may be installed on a
      front-end machine and mounted on every node of the cluster (ideally also
      to <filename>/opt/parastation</filename>). Be aware of the fact that even
      if &ps; is installed on a front-end machine, some configuration work on
      each node has to be done anyway.
    </para>
    <para>
      Within the discussion of the different installation methods below both
      modes will be described, the <quote>distributed</quote> and the
      <quote>centralized</quote> one.
    </para>
    <para>
      From the authors point of view the benefits of the centralized
      installation outweigh the disadvantages and furthermore the advantages of
      the distributed installation. Thus putting &ps; on a NFS server and
      mounting the directory on all the nodes is the method of choice.
    </para>
    <note>
      <para>
      Experience has shown that mounting &ps; or even
	<filename>/home</filename> via NFS using <citerefentry>
	  <refentrytitle>automount</refentrytitle>
	  <manvolnum>8</manvolnum>
      </citerefentry> may cause major problems during the startup of &ps; or
	even on application startup. This is due to the fact that requests of
	the automount daemons to the NFS server might timeout if many clients
	try to mount directories almost concurrently. On the other hand there
	is no real need using
      <citerefentry>
	  <refentrytitle>automount</refentrytitle>
	  <manvolnum>8</manvolnum>
      </citerefentry> in this case, since the nodes of a cluster usually cannot
	be operated in a usable way without the users'
	<filename>/home</filename> directories. Therefore static NFS mounting
	seems to be reasonable in this case.
      </para>
    </note>
    <section id="dir_struct">
      <title>Directory structure</title>
      <para>
	The default location to install &ps; is
	<filename>/opt/parastation</filename>. Underneath this directory,
	several subdirectories are created containing the actual &ps;
	installation:
      </para>
      <variablelist>
	<varlistentry>
	  <term>
	    <filename>bin</filename>
	  </term>
	  <listitem>
	    <para>contains all executables and scripts forming the &ps; system.
	      Remember to include this directory into the <envar>PATH</envar>
	      environment.
	    </para>
	  </listitem>
	</varlistentry>
	<varlistentry>
	  <term>
	    <filename>bin/modules</filename>
	  </term>
	  <listitem>
	    <para>contains various subdirectories with the kernel modules
	      needed for the high throughput, low latency communication. Each
	      subdirectory holds modules for a specific kernel that is
	      supported by the standard distribution.
	    </para>
	  </listitem>
	</varlistentry>
	<varlistentry>
	  <term>
	    <filename>config</filename>
	  </term>
	  <listitem>
	    <para>contains the installation script
	      <filename>PSM_INSTALL</filename> and the example configuration
	      file <filename>parastation.conf.tmpl</filename>.
	    </para>
	    <para>
	      Furthermore within the standard installation the main
	      configuration file <filename>parastation.conf</filename> is
	      physically located here. Through a symbolic link this
	      configuration file is also available in
	      <filename>/etc</filename>, where it is expected from the &ps;
	      daemon <xref linkend="psid"/> by default. This symbolic link
	      will be created by <command>PSM_INSTALL</command> during
	      installation.
	    </para>
	    <para>
	      Depending on the communication part of the &ps; system installed
	      further scripts and configuration files may be found within this
	      directory. E.g. if the Myrinet communication system is installed,
	      some example routing files for the &ps; Myrinet driver and the
	      setup script <command>ps_myrinet</command> are located
	      here.
	    </para>
	  </listitem>
	</varlistentry>
	<varlistentry>
	  <term>
	    <filename>doc</filename>
	  </term>
	  <listitem>
	    <para>contains the &ps; documentation after installing the
	      corresponding RPM or <filename>.tar.gz</filename> file. The
	      necessary steps are described in <xref linkend="docu_install"/>.
	    </para>
	  </listitem>
	</varlistentry>
	<varlistentry>
	  <term>
	    <filename>include</filename>
	  </term>
	  <listitem>
	    <para>contains the header files needed in order to build &ps;
	      applications. These are primarily needed for building
	      applications using the low level PSPort or PSE libraries.
	    </para>
	    <para>
	      These header files are not needed, if only MPI application should
	      be build or precompiled third party applications are used.
	    </para>
	  </listitem>
	</varlistentry>
	<varlistentry>
	  <term>
	    <filename>lib</filename>
	  </term>
	  <listitem>
	    <para>contains various libraries needed in order to build &ps;
	      applications. These are primarily needed for building MPI
	      applications or such using the low level PSPort or PSE libraries.
	    </para>
	  </listitem>
	</varlistentry>
	<varlistentry>
	  <term>
	    <filename>man</filename>
	  </term>
	  <listitem>
	    <para>contains the manual pages describing the &ps; daemons,
	      utilities and configuration files after installing the
	      documentation RPM or <filename>.tar.gz</filename> file. The
	      necessary steps are described in <xref linkend="docu_install"/>.
	    </para>
	    <para>
	      In order to enable these pages to the user via the
	      <citerefentry>
		<refentrytitle>man</refentrytitle>
		<manvolnum>1</manvolnum>
	    </citerefentry> command, please consult the corresponding
	      documentation<footnote>
		<para>
		  Usually this is either done by modifying the
		  <envar>MANPATH</envar> environment variable or by editing the
		  <citerefentry>
		    <refentrytitle>manpath</refentrytitle>
		    <manvolnum>1</manvolnum>
		  </citerefentry> configuration file, which is
		  <filename>/etc/manpath.config</filename> by default.
		</para>
	      </footnote>.
	    </para>
	  </listitem>
	</varlistentry>
	<varlistentry>
	  <term>
	    <filename>mpich</filename>
	  </term>
	  <listitem>
	    <para>contains a adapted version of <ulink
		url="http://www.unix.mcs.anl.gov/mpi/mpich">MPIch</ulink> after
	      installing one of the various <filename>mpich-ps</filename> RPM
	      or <filename>.tar.gz</filename> files. The necessary steps are
	      described in <xref linkend="mpich_install"/>.
	    </para>
	  </listitem>
	</varlistentry>
      </variablelist>
    </section>
  </section>

  <section>
    <title>Installation via rpm</title>
    <para>
      The easiest way to install &ps; is the installation via RPM. This is the
      preferred method on all <ulink url="http://www.suse.de">SuSE</ulink> or
      <ulink url="http://www.redhat.com">RedHat</ulink> based systems.
    </para>
    <simplesect id="Get_RPMs">
      <title>Getting the &ps; RPM files</title>
      <para>
	RPMs containing the various versions of the &ps; system can be obtained
	from the <ulink url="http://www.par-tec.com/en/download.php">download
	  section</ulink> of the <ulink url="http://www.par-tec.com">ParTec
	  homepage</ulink>.
      </para>
      <para>
	Usually two different files are needed, one containing the management
	part, another one with communication part of the &ps; system. Of course
	the communication part to choose will depend on the interconnection
	hardware in use and the license version purchased.
      </para>
      <para>
	The full names of the RPM files follow a simple structure:
      </para>
      <programlisting>name-x.y.z-yyyymmdd.arch.rpm</programlisting>
      <para>
	Here <filename>name</filename> denotes the name and thus the content of
	the packet. Furthermore <filename>x.y.z</filename> is the version
	number, <filename>yyyymmdd</filename> the release date and
	<filename>arch</filename> is the architecture, i.e. one of
	<filename>i386</filename>, <filename>ia64</filename>,
	<!--<filename>x86-64</filename>, --> <filename>alpha</filename> or
	<filename>noarch</filename>. The latter is used e.g. for the
	documentation packet.
      </para>
      <para>
	The name of the packet holding the management part is
	<filename>psmgmt</filename>. This packets is needed for any
	installation of the &ps; system, no matter what is the underlying
	communication platform.
      </para>
      <para>
	The name of the RPM containing the communication part depends on the
	supported communication hardware. For the Myrinet version of &ps; this
	is <filename>psm</filename>, the communication part of &psfe; is called
	<filename>psfe</filename> and the corresponding packets of &ps4; holds
	the name <filename>pscom</filename>. Like in the management RPM all
	these filenames are followed by an individual version number, the
	release date and the architecture.
      </para>
      <para>
	The versions you find on the <ulink url="http://www.par-tec.com">ParTec
	  homepage</ulink> at a distinct point of time will work together
	without problem. If you want to update only part of your installation
	(i.e. only the management part while keeping the communication part
	untouched) you have to have a look at the corresponding release notes
	in order to find out if the intended combination actually works.
      </para>
      <para>
	The release notes of the different packages will either be found within
	the <filename>/opt/parastation</filename> directory or on the <ulink
	  url="http://www.par-tec.com">ParTec homepage</ulink>.
      </para>
      <para>
	 Please note that the individual version numbers of the distinct
	packets building the &ps; system do not necessarily have to
	match. E.g. if you want to install a &psfe; system, the management part
	might have version number <filename>4.0.1</filename> while the
	communication part still has version number
	<filename>1.0.3</filename>.
      </para>
    </simplesect>
    <simplesect>
      <title>Centralized installation</title>
      <para>
	For the centralized installation, become <quote>root</quote> on the
	machines choosen to be the fileserver, i.e. usually on the front-end
	machine.
      </para>
      <para>
	Then install the management part and the choosen communication part of
	&ps; using the <command>rpm -U</command> command. This will install all
	the necessary file to <filename>/opt/parastation</filename> and call
	the script <command>PSM_INSTALL</command> residing in the
	<filename>config</filename> directory via:
      </para>
      <programlisting>	/opt/parastation/config/PSM_INSTALL -i</programlisting>
      <para>
	This will modify some configuration files in <filename>/etc</filename>.
	The modifications done by this script are discussed in some detail in
	<xref linkend="Install_by_hand"/>.
      </para>
      <para>
	Then make sure that <filename>/opt/parastation</filename> is exported
	and the NFS server is running (use <citerefentry>
	  <refentrytitle>showmount</refentrytitle>
	  <manvolnum>8</manvolnum>
	</citerefentry> to do so). Mount the exported directory on all nodes to
	<filename>/opt/parastation</filename>. Afterwards run
      </para>
      <programlisting>	/opt/parastation/config/PSM_INSTALL -i</programlisting>
      <para>
	manually on every nodes of the cluster.
      </para>
    </simplesect>
    <simplesect> 
      <title>Distributed installation</title>
      <para>
	In order to install &ps; from RPM in a distributed fashion, you have to
	install the two RPMs on all the nodes of the cluster. Therefore become
	<quote>root</quote> and use the command <command>rpm -U</command> to
	actually install the RPMs. These two step have to be executed on any
	node of the cluster including the front-end machine.
      </para>
      <para>
	Since the <command>PSM_INSTALL</command> is called within the post
	install script of the RPMs, it is not necessary to run it explicitely.
      </para>
      <para>
	The modifications done by this script to some of the files located in
	the <filename>/etc</filename> directory are discussed in <xref
	  linkend="Install_by_hand"/>.
      </para>
    </simplesect>
  </section>

  <section>
    <title>Installation using <filename>.tar.gz</filename> files</title>
    <para>
      If the linux distribution in use does not support RPM, the apropriate way
      to install &ps; is the one using the <filename>.tar.gz</filename> files.
    </para>
    <simplesect>
      <title>Getting the &ps; <filename>.tar.gz</filename> files</title>
      <para>
	The <filename>.tar.gz</filename> files containing &ps; can be obtained
	from the <ulink url="http://www.par-tec.com/en/download.php">download
	  section</ulink> of the <ulink url="http://www.par-tec.com">ParTec
	  homepage</ulink>.
      </para>
      <para>
	The names of the <filename>.tar.gz</filename> files containing the
	different components of the &ps; software follow suit the conventions
	of the RPM files described in <xref linkend="Get_RPMs"/>. Only the
	<filename>.rpm</filename> extension is replaced by the
	<filename>.tar.gz</filename> one.
      </para>
      <para>
	As in the RPM case, here you will also need two different files, one
	containing the management part of the &ps; system and one comprising
	the communication part. While the management packet holds the unique
	name <filename>psmgmt</filename>, the name of the communication packet
	depends on the communication hardware in use. It is either
	<filename>psm</filename> for the Myrinet hardware,
	<filename>psfe</filename> for the &psfe; software or
	<filename>pscom</filename> for &ps4;.
      </para>
      <para>
	Again you can learn more about which versions of the management and
	communication part will play together by having a brief view on the
	release notes coming with each packet.
      </para>
      <para>
	As a rule of thumbs the packets that can be fetched from the download
	section at a distinct time will play together without problem.
      </para>
    </simplesect>
    <simplesect>
      <title>Centralized installation</title>
      <para>
	The installation of &ps; from <filename>.tar.gz</filename> files in a
	centralized fashion is done in the following steps: Become
	<quote>root</quote> and change to the root directory
	<filename>/</filename> on the NFS server machine. Then use the command
      </para>
      <programlisting>	gzip -cd psmgmt-x.y.z-yyyymmdd.arch.tar.gz | tar xvf -</programlisting>
      <para>
	in order to put all the files of &ps; managment system onto disk, i.e.
	to <filename>/opt/parastation</filename>. Enabling the &ps; management
	facility on this machine is done by executing the script
	<command>PSM_INSTALL</command> residing in &ps;'s
	<filename>config</filename> directory:
      </para>
      <programlisting>	/opt/parastation/config/PSM_INSTALL -i</programlisting>
      <para>
	The modifications done by this script will be discussed in <xref
	  linkend="Install_by_hand"/>.
      </para>
      <para>
	In the next step &ps;'s communication part has to be installed in the
	same fashion. Assuming the <filename>.tar.gz</filename> file holding
	your communcation part is name <filename>comm.tar.gz</filename>,
	execute:
      </para>
      <programlisting>	gzip -cd comm.tar.gz | tar xvf -</programlisting>
      <para>
	in order to install this part of the &ps; system.
      </para>
      <para>
	Now make sure that <filename>/opt/parastation</filename> is exported
	and the NFS server is running (use <citerefentry>
	  <refentrytitle>showmount</refentrytitle>
	  <manvolnum>8</manvolnum>
	</citerefentry> to do so). Then mount the exported directory on all
	nodes to <filename>/opt/parastation</filename> and run
      </para>
      <programlisting>	/opt/parastation/config/PSM_INSTALL -i</programlisting>
      <para>
	manually on all this nodes.
      </para>
    </simplesect>
    <simplesect>
      <title>Distributed installation</title>
      <para>
	In the case of a distributed installation become <quote>root</quote>
	and change to the root directory <filename>/</filename>. Then just do a
      </para>
      <programlisting>	gzip -cd psmgmt-x.y.z-yyyymmdd.arch.tar.gz | tar xvf -</programlisting>
      <para>
	in order to install &ps;'s management part and a similar line for the
	communication libraries, kernel modules etc. needed for your hardware
	setup.
      </para>
      <para>
	This will install all the necessary file to
	<filename>/opt/parastation</filename>. Afterwards the script
	<command>PSM_INSTALL</command> residing in the
	<filename>config</filename> directory has to be called:
      </para>
      <programlisting>	/opt/parastation/config/PSM_INSTALL -i</programlisting>
      <para>
	This will modify some configuration files in <filename>/etc</filename>.
	The modifications done by this script are discussed in detail in <xref
	  linkend="Install_by_hand"/>.
      </para>
      <para>
	These steps have to be executed on all nodes of the cluster.
      </para>
    </simplesect>

  </section>

  <section id="Install_by_hand">
    <title>Installation by hand</title>
    <para>
      For full control on the installation process you might not want to run
      the <command>PSM_INSTALL</command> script but do the necessary
      modifications by hand. Therefore the changes made by this script will be
      discussed now step by step.
    </para>
    <note>
      <para>
	You have to be root to execute the following steps.
      </para>
    </note>

    <orderedlist>
      <listitem id="ibh_create_dev">
	<para><emphasis role="bold">Create the device entry</emphasis></para>
	<para>
	  If not yet done (e.g. from a previous installation), you must create
	  a character device:
	</para>
	<programlisting>	mknod /dev/psm c 40 0
	chmod 666 /dev/psm</programlisting>
	<note>
	  <para>
	    This has only to be done for the Myrinet version of &ps;. You can
	    omit this step within the installation procedure of &psfe;.
	  </para>
	</note>
      </listitem>
      <listitem id="ibh_services">
	<para><emphasis role="bold">Services</emphasis></para>
	<para>
	  The following lines must be appended to
	  <filename>/etc/services</filename>:
	</para>
	<programlisting>
	# ParaStation entries
	psld          887/tcp     # ParaStation License Daemon Start Port
	psid          888/tcp     # ParaStation Daemon Start Port
	# end of ParaStation entries</programlisting>
	<para>
	  These ports are used by the automatic startup mechanism of the &ps;
	  daemon <xref linkend="psid"/> and license daemon <xref
	    linkend="psld"/> via <citerefentry>
	    <refentrytitle>inetd</refentrytitle>
	    <manvolnum>8</manvolnum>
	  </citerefentry> or <citerefentry>
	    <refentrytitle>xinetd</refentrytitle>
	    <manvolnum>8</manvolnum>
	  </citerefentry>.
	</para>
      </listitem>
      <listitem id="ibh_inetd">
	<para><emphasis role="bold">(X)inetd Configuration</emphasis></para>
	<para>
	  This part of the installation process differs for the various Linux
	  distributions. If your system has
	  <itemizedlist>
	    <listitem>
	      <para>a file named <filename>/etc/inetd.conf</filename> (e.g. a
		SuSE distributions prior to 8.2), read section <xref
		  linkend="ibh_inetd_suse"/>,
	      </para>
	    </listitem>
	    <listitem>
	      <para>a directory <filename>/etc/xinetd.d</filename> (e.g. a
		RedHat distribution or a SuSE 8.2 or higher), read section
		<xref linkend="ibh_inetd_rh"/>.
	      </para>
	    </listitem>
	  </itemizedlist>
	</para>
	<orderedlist numeration="arabic" inheritnum="inherit">
	  <listitem id="ibh_inetd_suse">
	    <para><emphasis role="bold">Inetd Configuration</emphasis></para>
	    <para>
	      The following lines must be appended to
	      <filename>/etc/inetd.conf</filename>:
	    </para>
	    <programlisting>
	#
	# ParaStation daemon
	#
	psid stream tcp nowait root /opt/parastation/bin/psid psid
	#
	# ParaStation license server
	#
	psld stream tcp nowait root /opt/parastation/bin/psld psld</programlisting>
	    <para>
	      Now, the <citerefentry>
		<refentrytitle>inetd</refentrytitle>
		<manvolnum>8</manvolnum>
	      </citerefentry>
	      must be triggered to reload this file:
	    </para>
	    <programlisting>	/etc/init.d/inetd reload</programlisting>
	    <para>
	      or send the signal <filename>SIGHUP</filename> to the daemon.
	    </para>
	  </listitem>
	  <listitem id="ibh_inetd_rh">
	    <para><emphasis role="bold">Xinetd Configuration</emphasis></para>
	    <para>
	      Create a file named
	      <filename>/etc/xinetd.d/parastation</filename> with the following
	      contents:
	    </para>
	    <programlisting>
	#
	# ParaStation daemon
	#
	service psid
	{
	disable         = no
	socket_type     = stream
	wait            = no
	user            = root
	server          = /opt/parastation/bin/psid
	server_args     =
	log_on_failure  += USERID
	}
	#
	# ParaStation license server
	#
	service psld
	{
	disable         = no
	socket_type     = stream
	wait            = no
	user            = root
	server          = /opt/parastation/bin/psld
	server_args     =
	log_on_failure  += USERID
	}</programlisting>
	    <para>
	      Now, the xinetd daemon must be triggered to read this file:
	    </para>
	    <programlisting>	/etc/init.d/xinetd reload</programlisting>
	  </listitem>
	</orderedlist>
      </listitem>
    </orderedlist>
  </section>

  <section id="docu_install">
    <title>Installation of the documentation</title>
    <para>
      The &ps; documentation is delivered in three formats: As PDF files, a
      browsable HTML documentation and manual pages for all &ps; programs and
      configuration files.
    </para>
    <para>
      In order to install the documentation files first of all an up to date
      version of the documentation has to be retrieved. It can be found within
      the <ulink url="http://www.par-tec.com/en/download.php">download
	section</ulink> of the <ulink url="http://www.par-tec.com">ParTec
	homepage</ulink>. The names of the rpm of <filename>.tar.gz</filename>
      files follow the conventions of all other files building the &ps;
      distribution. Here the name part is <filename>ps-doc</filename>and the
      architecure is <filename>noarch</filename>, since this part is platform
      independent.
    </para>
    <para>
      For the installation of the rpm file, simply execute 
    </para>
    <programlisting>	rpm -U ps-doc-...-noarch.rpm</programlisting>
    <para>
      Similary for the <filename>.tar.gz</filename> file change to the root
      directory <filename>/</filename> and execute
    </para>
    <programlisting>	gzip -cd ps-doc-....tar.gz | tar xvf -</programlisting>
    <para>
      All the PDF and HTML files of the documentation will be installed into
      <filename>/opt/parastation/doc</filename>, the manual pages will reside
      in <filename>/opt/parastation/man</filename>.
    </para>
    <para>
      The intended starting point to browse the HTML version of the
      documentation is
      <filename>file:///opt/parastation/doc/html/index.html</filename>.
    </para>
    <para>
      The PDF documentation is included in two PDF file called
      <filename>adminguide.pdf</filename> for the <emphasis role="bold">&ps;
	Administrators Guide</emphasis> and <filename>userguide.pdf</filename>
      for the <emphasis role="bold">&ps; Users Guide</emphasis>. Both may be
      found in the <filename>/opt/parastation/doc/pdf</filename> directory. The
      PDF documentation can be viewed using the Acrobat Reader, which can be
      retrieved from the <ulink url="http://www.adobe.com">Adobe
	homepage</ulink>.
    </para>
    <para>
      Hardcopies of the &ps; documentation can be obtained from ParTec,
      although it is much easier for the customer to just print out the PDF
      file using the Acrobat Reader.
    </para>
    <para>
      In order to enable the manual pages to the users please consult the
      documentation of the <citerefentry>
	<refentrytitle>man</refentrytitle>
	<manvolnum>1</manvolnum>
      </citerefentry> command and the remark in <xref linkend="dir_struct"/> on
      how to do this.
    </para>
  </section>

  <section id="mpich_install">
    <title>Installation of MPIch</title>
    <para>The standard for the implementation of parallel applications on
      distributed memory machines like clusters is MPI, the Message Passing
      Interface. In order to enable a &ps; cluster for the development and
      execution of MPI programs, the installation of an adapted version of
      <ulink url="http://www-unix.mcs.anl.gov/mpi/mpich/">MPIch</ulink> is
      necessary. A corresponding rpm of <filename>.tar.gz</filename> packet can
      be found within the <ulink
	url="http://www.par-tec.com/en/download.php">download section</ulink>
      of the <ulink url="http://www.par-tec.com">ParTec homepage</ulink>.
    </para>
    <para>
      The names of the correspondings files follow the common naming
      conventions of all &ps; packets. The <filename>name</filename> part
      discussed above is <filename>mpich-ps</filename> for &ps3; on Myrinet or
      <filename>mpich-ps4</filename> for &ps4; or &psfe;.<footnote>
	<para>There exist further version of the MPIch packet that are build
	  with different compilers like the PGI or Intel compilers on the Intel
	  IA32 platform, the Intel compiler on the Intel IA64 platform or the
	  Compaq compiler on the Alpha platform. These packets of course will
	  depend on the corresponding compilers to be installed. Keep in mind
	  that the use of this compilers might require further licenses.
	</para>
      </footnote>
    </para>
    <note>
      <para>
	The version number of the MPIch packet denotes the version of the MPIch
	source code used in order to create the libraries. The versions of the
	&ps; software a distinct packet plays with can be determined from the
	<filename>ReleasNotes</filename> file contained within the rpm or
	<filename>.tar.gz</filename> file.
      </para>
    </note>
    <para>
      After downloading the correct MPIch distribution file make sure to be
      root. In order to install MPIch for &ps; from the rpm file, simply
      execute:
    </para>
    <programlisting>	rpm -U mpich-ps[4]-....rpm</programlisting>
    <para>
      If you have decided to use the <filename>.tar.gz</filename> file, you
      have to change to the root directory <filename>/</filename> and use
    </para>
    <programlisting>	gzip -cd mpich-ps[4]-....tar.gz | tar xvf -</programlisting>
    <para>
      instead. Both commands will extract the &ps;/MPIch distribution to
      <filename>/opt/parastation/mpich</filename>. Since all these files are
      only needed for building MPI applications, but not for running them, it
      is sufficient to do this installation only on the nodes used for
      compilation or the front-end machine.
    </para>
    <para>
      In order to enable the MPI commands to the users make sure that
      <filename>/opt/parastation/mpich/bin</filename> is included into the
      system wide <envar>PATH</envar> environment variable. Furthermore you
      might want to enable the MPI man pages. These reside in
      <filename>/opt/parastation/mpich/man</filename>. Please consult the
      documentation of the <citerefentry>
	<refentrytitle>man</refentrytitle>
	<manvolnum>1</manvolnum>
      </citerefentry> command and the remark in <xref linkend="dir_struct"/> on
      how to do this.
    </para>
  </section>

  <section>
    <title>Further steps</title>
    <para>
      With this the basic installation of &ps; is done. There remain some
      further step until &ps; can really be used. These include:
    </para>
    <itemizedlist>
      <listitem>
	<para><link linkend="config_ps">the configuration of the &ps;
	    system</link>
	</para>
      </listitem>
      <listitem>
	<para><link linkend="setup_route">setting up the routing for Myrinet
	    communication</link>
	</para>
      </listitem>
      <listitem>
	<para><link linkend="testing">testing if everything works well</link>
	</para>
      </listitem>
      <listitem>
	<para><link linkend="embed">optionally the embedding of the &ps; system
	    into an already existing batch system.</link>
	</para>
      </listitem>
    </itemizedlist>
    <para>
      These step will be discussed in the next chapter of this document.
    </para>
  </section>

  <section>
    <title>Uninstall</title>
    <para>
      In order to remove &ps; from the cluster it was installed on, there are
      different ways to do so depending on the choosen way of installation.
    </para>
    <simplesect>
      <title>Centralized installations</title>
      <para>
	First of all for any centralized installation you have to run
      </para>
      <programlisting>	/opt/parastation/config/PSM_INSTALL -u</programlisting>
      <para>
	on all nodes that have mounted <filename>/opt/parastation</filename>
	from the NFS server unless you want to remove the modifications done
	while installation manually. In that case consult <xref
	linkend="Install_by_hand"/> to find out which steps have to be made
	undone.
      </para>
      <para>
      Now it is save to remove all files installed on the the NFS server. If
	the installation was made via <citerefentry>
	  <refentrytitle>rpm</refentrytitle>
	  <manvolnum>1</manvolnum>
      </citerefentry> the corresponding packets simply have to be remove using
      </para>
      <programlisting>	rpm -e psmgmt pscomm psm psfe ps-doc</programlisting>
      <para>
	For installations done from the <filename>.tar.gz</filename> first undo
	all modification to configuration files within
	<filename>/etc</filename> by running
      </para>
      <programlisting>	/opt/parastation/config/PSM_INSTALL -u</programlisting>
      <para>
	Afterwards /opt/parastation and all its subdirectories can be removed
	savely.
      </para>
    </simplesect>
    <simplesect>
      <title>Distributed installation</title>
      <para>
	If the installation was done from the RPM file, simply execute a
      </para>
      <programlisting>	rpm -e psmgmt pscomm psm psfe ps-doc</programlisting>
      <para>
	on all nodes of the cluster. Otherwise
      </para>
      <programlisting>	/opt/parastation/config/PSM_INSTALL -u</programlisting>
      <para>
	has to be called on every node of the cluster and
	<filename>/opt/parastation</filename> can be removed afterwards.
      </para>
    </simplesect>
  </section>
</chapter>
  <!-- Keep this comment at the end of the file
  Local variables:
  mode: xml
  sgml-omittag:nil
  sgml-shorttag:nil
  sgml-namecase-general:nil
  sgml-general-insert-case:lower
  sgml-minimize-attributes:nil
  sgml-always-quote-attributes:t
  sgml-indent-step:2
  sgml-indent-data:t
  sgml-parent-document:("adminguide.xml" "book" "book" ("title" "bookinfo"))
  sgml-exposed-tags:nil
  sgml-local-catalogs:nil
  sgml-local-ecat-files:nil
  End:
  -->
